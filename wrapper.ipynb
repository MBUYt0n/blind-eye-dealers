{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 64424,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 53721
        },
        {
          "sourceId": 60178,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 50333
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'emogen2/tensorflow2/one/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F53721%2F64424%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240613%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240613T132419Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da274f4e22fc82aa42ef2bd6731c6d2033b1d6ecf9240ba2a39989384ef7f6a49300e2c38c10c4cb235078f88bd815acf60b8249e16ce719a95388786cfc98f5d97802a3cd7824d16ecdf6acec387a6d0b223959fda03c3a0e95e1f0dfea8fefc83e4db5de89a7dd1fb3b241c94b2c05f53d6546f4292342b19f9afdb98a4f524365e3a2a7a9b7dbec8c1030d23b5a436a406a9fef7029251fd1521670b2f364fd2634c412170fd85cb622978eed2ff3348ed258ddabfd95fd72984968aa01802b4557768f3c6868c7b72f5c53fa5a152e8a9fea3f55285f1d0715b254b38fb0a338b0a135635f4690d81724ce333ac67c203165224b401e17f9296c433be87bc,yolov8/pytorch/open-image-v8/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F50333%2F60178%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240613%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240613T132420Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1e6139bd454ddc39527c6e4062138f8b3b2ce1f8a454ae59f4952c82199e4cd67c811b5d4214568f25602af5657b683f1e7624683aa23c6324e4b48f3af49be303a8a6e11a4e64318078671019067e9154e3e1be9b615d3d3a0030953e66facbb9392332a34cc69bea67d7d6234da21d6ef240e8a779e787c3c1c6858593e238a2f24e667ded02600c6feffb78195034cc8bc2457a925790469787bf3d2299179624020d0344aad29315c5f9bab2cb9c2f860fb22c836c6ea9093f32f979eb7dd5420485c25b919581e693f496031c7b17c9acf3a08de1d02dcc353ef91c34510a6f872894767485aa0b0da00565f63aa90e2dd32eb5f3ede6a401c8ffb1b2a7'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "XIxFk-xVxVOM",
        "outputId": "bd87e8ed-91a3-4b05-b991-3e4bb56ccdc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading emogen2/tensorflow2/one/1, 203010682 bytes compressed\n",
            "[==================================================] 203010682 bytes downloaded\n",
            "Downloaded and uncompressed: emogen2/tensorflow2/one/1\n",
            "Downloading yolov8/pytorch/open-image-v8/1, 21264265 bytes compressed\n",
            "[==================================================] 21264265 bytes downloaded\n",
            "Downloaded and uncompressed: yolov8/pytorch/open-image-v8/1\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics lapx easyocr pytube dlib"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-06-13T12:55:36.04466Z",
          "iopub.execute_input": "2024-06-13T12:55:36.045766Z",
          "iopub.status.idle": "2024-06-13T13:04:14.240029Z",
          "shell.execute_reply.started": "2024-06-13T12:55:36.045716Z",
          "shell.execute_reply": "2024-06-13T13:04:14.238418Z"
        },
        "trusted": true,
        "id": "vjg41KcUxVON",
        "outputId": "9aa35460-15c1-43cb-bb68-9005e796ed00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.31-py3-none-any.whl (780 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.6/780.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lapx\n",
            "  Downloading lapx-0.5.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting easyocr\n",
            "  Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=0.2.5 (from ultralytics)\n",
            "  Downloading ultralytics_thop-0.2.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lapx) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.82)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.3)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.4)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi->easyocr) (1.16.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.5.22)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (1.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: pyclipper, ninja, pytube, python-bidi, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lapx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics, easyocr\n",
            "Successfully installed easyocr-1.7.1 lapx-0.5.9 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pyclipper-1.3.0.post5 python-bidi-0.4.2 pytube-15.0.0 ultralytics-8.2.31 ultralytics-thop-0.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Activation, add, Dense, Flatten, Dropout, Conv2D, AveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "sys.setrecursionlimit(2 ** 20)\n",
        "np.random.seed(2 ** 10)\n",
        "\n",
        "\n",
        "class WideResNet:\n",
        "    def __init__(self, image_size, depth=16, k=8):\n",
        "        self._depth = depth\n",
        "        self._k = k\n",
        "        self._dropout_probability = 0\n",
        "        self._weight_decay = 0.0005\n",
        "        self._use_bias = False\n",
        "        self._weight_init = \"he_normal\"\n",
        "\n",
        "        if K.image_data_format() == \"channels_first\":\n",
        "            logging.debug(\"image_dim_ordering = 'th'\")\n",
        "            self._channel_axis = 1\n",
        "            self._input_shape = (3, image_size, image_size)\n",
        "        else:\n",
        "            logging.debug(\"image_dim_ordering = 'tf'\")\n",
        "            self._channel_axis = -1\n",
        "            self._input_shape = (image_size, image_size, 3)\n",
        "\n",
        "    # Wide residual network http://arxiv.org/abs/1605.07146\n",
        "    def _wide_basic(self, n_input_plane, n_output_plane, stride):\n",
        "        def f(net):\n",
        "            # format of conv_params:\n",
        "            #               [ [kernel_size=(\"kernel width\", \"kernel height\"),\n",
        "            #               strides=\"(stride_vertical,stride_horizontal)\",\n",
        "            #               padding=\"same\" or \"valid\"] ]\n",
        "            # B(3,3): orignal <<basic>> block\n",
        "            conv_params = [[3, 3, stride, \"same\"],\n",
        "                           [3, 3, (1, 1), \"same\"]]\n",
        "\n",
        "            n_bottleneck_plane = n_output_plane\n",
        "\n",
        "            # Residual block\n",
        "            for i, v in enumerate(conv_params):\n",
        "                if i == 0:\n",
        "                    if n_input_plane != n_output_plane:\n",
        "                        net = BatchNormalization(axis=self._channel_axis)(net)\n",
        "                        net = Activation(\"relu\")(net)\n",
        "                        convs = net\n",
        "                    else:\n",
        "                        convs = BatchNormalization(axis=self._channel_axis)(net)\n",
        "                        convs = Activation(\"relu\")(convs)\n",
        "\n",
        "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
        "                                          strides=v[2],\n",
        "                                          padding=v[3],\n",
        "                                          kernel_initializer=self._weight_init,\n",
        "                                          kernel_regularizer=l2(self._weight_decay),\n",
        "                                          use_bias=self._use_bias)(convs)\n",
        "                else:\n",
        "                    convs = BatchNormalization(axis=self._channel_axis)(convs)\n",
        "                    convs = Activation(\"relu\")(convs)\n",
        "                    if self._dropout_probability > 0:\n",
        "                        convs = Dropout(self._dropout_probability)(convs)\n",
        "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
        "                                          strides=v[2],\n",
        "                                          padding=v[3],\n",
        "                                          kernel_initializer=self._weight_init,\n",
        "                                          kernel_regularizer=l2(self._weight_decay),\n",
        "                                          use_bias=self._use_bias)(convs)\n",
        "\n",
        "            # Shortcut Connection: identity function or 1x1 convolutional\n",
        "            #  (depends on difference between input & output shape - this\n",
        "            #   corresponds to whether we are using the first block in each\n",
        "            #   group; see _layer() ).\n",
        "            if n_input_plane != n_output_plane:\n",
        "                shortcut = Conv2D(n_output_plane, kernel_size=(1, 1),\n",
        "                                         strides=stride,\n",
        "                                         padding=\"same\",\n",
        "                                         kernel_initializer=self._weight_init,\n",
        "                                         kernel_regularizer=l2(self._weight_decay),\n",
        "                                         use_bias=self._use_bias)(net)\n",
        "            else:\n",
        "                shortcut = net\n",
        "\n",
        "            return add([convs, shortcut])\n",
        "\n",
        "        return f\n",
        "\n",
        "\n",
        "    # \"Stacking Residual Units on the same stage\"\n",
        "    def _layer(self, block, n_input_plane, n_output_plane, count, stride):\n",
        "        def f(net):\n",
        "            net = block(n_input_plane, n_output_plane, stride)(net)\n",
        "            for i in range(2, int(count + 1)):\n",
        "                net = block(n_output_plane, n_output_plane, stride=(1, 1))(net)\n",
        "            return net\n",
        "\n",
        "        return f\n",
        "\n",
        "#    def create_model(self):\n",
        "    def __call__(self):\n",
        "        logging.debug(\"Creating model...\")\n",
        "\n",
        "        assert ((self._depth - 4) % 6 == 0)\n",
        "        n = (self._depth - 4) / 6\n",
        "\n",
        "        inputs = Input(shape=self._input_shape)\n",
        "\n",
        "        n_stages = [16, 16 * self._k, 32 * self._k, 64 * self._k]\n",
        "\n",
        "        conv1 = Conv2D(filters=n_stages[0], kernel_size=(3, 3),\n",
        "                              strides=(1, 1),\n",
        "                              padding=\"same\",\n",
        "                              kernel_initializer=self._weight_init,\n",
        "                              kernel_regularizer=l2(self._weight_decay),\n",
        "                              use_bias=self._use_bias)(inputs)  # \"One conv at the beginning (spatial size: 32x32)\"\n",
        "\n",
        "        # Add wide residual blocks\n",
        "        block_fn = self._wide_basic\n",
        "        conv2 = self._layer(block_fn, n_input_plane=n_stages[0], n_output_plane=n_stages[1], count=n, stride=(1, 1))(conv1)\n",
        "        conv3 = self._layer(block_fn, n_input_plane=n_stages[1], n_output_plane=n_stages[2], count=n, stride=(2, 2))(conv2)\n",
        "        conv4 = self._layer(block_fn, n_input_plane=n_stages[2], n_output_plane=n_stages[3], count=n, stride=(2, 2))(conv3)\n",
        "        batch_norm = BatchNormalization(axis=self._channel_axis)(conv4)\n",
        "        relu = Activation(\"relu\")(batch_norm)\n",
        "\n",
        "        # Classifier block\n",
        "        pool = AveragePooling2D(pool_size=(8, 8), strides=(1, 1), padding=\"same\")(relu)\n",
        "        flatten = Flatten()(pool)\n",
        "        predictions_g = Dense(units=2, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
        "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\",\n",
        "                              name=\"pred_gender\")(flatten)\n",
        "        predictions_a = Dense(units=101, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
        "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\",\n",
        "                              name=\"pred_age\")(flatten)\n",
        "        model = Model(inputs=inputs, outputs=[predictions_g, predictions_a])\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T13:08:29.069801Z",
          "iopub.execute_input": "2024-06-13T13:08:29.070292Z",
          "iopub.status.idle": "2024-06-13T13:08:43.195645Z",
          "shell.execute_reply.started": "2024-06-13T13:08:29.070252Z",
          "shell.execute_reply": "2024-06-13T13:08:43.194679Z"
        },
        "trusted": true,
        "id": "BNCCKnENxVOO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "video_url = 'https://www.youtube.com/watch?v=o4Zd-IeMlSY'\n",
        "\n",
        "yt = YouTube(video_url)\n",
        "\n",
        "video_stream = yt.streams.get_highest_resolution()\n",
        "\n",
        "output_path = 'downloaded_video.mp4'\n",
        "\n",
        "video_stream.download(filename=output_path)\n",
        "\n",
        "print(f\"Video downloaded and saved as {output_path}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T13:22:45.046912Z",
          "iopub.execute_input": "2024-06-13T13:22:45.047465Z",
          "iopub.status.idle": "2024-06-13T13:23:03.848177Z",
          "shell.execute_reply.started": "2024-06-13T13:22:45.04741Z",
          "shell.execute_reply": "2024-06-13T13:23:03.846858Z"
        },
        "trusted": true,
        "id": "Q76EoX8ZxVOO",
        "outputId": "af5d18ab-babd-4fd7-d92c-477968239379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video downloaded and saved as downloaded_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from ultralytics import YOLO\n",
        "import easyocr\n",
        "import torch\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import sys\n",
        "import io\n",
        "from transformers import pipeline\n",
        "\n",
        "class WrapperClass:\n",
        "    def __init__(self, img_size, depth, k, margin):\n",
        "        self.img_size = img_size\n",
        "        self.margin = margin\n",
        "        self.ag_model = WideResNet(img_size, depth=depth, k=k)()\n",
        "        self.ag_model.load_weights(\"/kaggle/input/emogen2/tensorflow2/one/1/weights.28-3.73.hdf5\")\n",
        "        self.emotion_model = load_model(\"/kaggle/input/emogen2/tensorflow2/one/1/emotion_little_vgg_2.h5\")\n",
        "        self.yolo = YOLO(\"/kaggle/input/yolov8/pytorch/open-image-v8/1/yolov8s-oiv7.pt\")\n",
        "        self.easyocr = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
        "        self.emotion_classes = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}\n",
        "        self.detector = dlib.get_frontal_face_detector()\n",
        "        self.corrector = pipeline('text2text-generation', model='vennify/t5-base-grammar-correction')\n",
        "\n",
        "    def detection(self, frame):\n",
        "        return self.detector(frame, 1)\n",
        "\n",
        "    def preprocess_face(self, img_h, img_w, detected, faces, frame):\n",
        "        preprocessed_faces_emo = []\n",
        "        if len(detected) > 0:\n",
        "            for i, d in enumerate(detected):\n",
        "                x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
        "                xw1 = max(int(x1 - self.margin * w), 0)\n",
        "                yw1 = max(int(y1 - self.margin * h), 0)\n",
        "                xw2 = min(int(x2 + self.margin * w), img_w - 1)\n",
        "                yw2 = min(int(y2 + self.margin * h), img_h - 1)\n",
        "                faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (self.img_size, self.img_size))\n",
        "                face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
        "                face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "                face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation = cv2.INTER_AREA)\n",
        "                face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\n",
        "                face_gray_emo = img_to_array(face_gray_emo)\n",
        "                face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\n",
        "                preprocessed_faces_emo.append(face_gray_emo)\n",
        "\n",
        "        return preprocessed_faces_emo, faces\n",
        "\n",
        "    def predict_ages(self, detected, faces):\n",
        "        if len(detected) > 0:\n",
        "            results = self.ag_model.predict(np.array(faces))\n",
        "            predicted_genders = results[0]\n",
        "            predicted_ages = np.argsort(results[1])[0][-5:]\n",
        "            age_range = (min(predicted_ages), max(predicted_ages))\n",
        "\n",
        "            return predicted_genders, age_range\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def predict_emo(self, detected, preprocessed_faces_emo):\n",
        "        emo_labels = []\n",
        "        for i, d in enumerate(detected):\n",
        "            preds = self.emotion_model.predict(preprocessed_faces_emo[i])[0]\n",
        "            e = self.emotion_classes[preds.argmax()]\n",
        "            emo_labels.append(e)\n",
        "        return emo_labels\n",
        "\n",
        "    def yolo_pred(self, frame):\n",
        "        res = self.yolo(frame)\n",
        "        return [(i['name'], i['box']) for j in res for i in j.summary()]\n",
        "\n",
        "    def correct_text(self, text):\n",
        "        return self.corrector(text)[0]['generated_text']\n",
        "\n",
        "\n",
        "    def vision_pred(self, frame):\n",
        "        preprocessed_faces_emo = []\n",
        "        input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        img_h, img_w, _ = np.shape(input_img)\n",
        "        detected = self.detector(frame, 1)\n",
        "        faces = np.empty((len(detected), self.img_size, self.img_size, 3))\n",
        "        preprocessed_faces_emo, faces = self.preprocess_face(img_h, img_w, detected, faces, frame)\n",
        "        gender, age = self.predict_ages(detected, faces)\n",
        "        emo_labels = self.predict_emo(detected, preprocessed_faces_emo)\n",
        "        print(gender, age, emo_labels)\n",
        "        result = self.yolo_pred(frame)\n",
        "        print(result)\n",
        "\n",
        "    def ocr_pred(self, frame):\n",
        "        result = self.easyocr.readtext(frame)\n",
        "        detected_text = \"\"\n",
        "        for item in result:\n",
        "            detected_text += \" \" + item[1]\n",
        "        corrected_text = self.correct_text(detected_text)\n",
        "        print(corrected_text)\n",
        "\n",
        "    def make_prediction(self, video_path, func):\n",
        "        a = time.time()\n",
        "        output_video_path = 'output.mp4'\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        frame_count = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % 30 != 0:\n",
        "                frame_count += 1\n",
        "                continue\n",
        "            if frame_count == 1500:\n",
        "              break\n",
        "\n",
        "            print(frame_count)\n",
        "            func(frame)\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        b = time.time()\n",
        "        print(b - a)\n",
        "\n",
        "    def __call__(self, video_path, func):\n",
        "        if func == \"see\":\n",
        "            self.make_prediction(video_path, self.vision_pred)\n",
        "        elif func == \"ocr\":\n",
        "            self.make_prediction(video_path, self.ocr_pred)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T13:23:07.48037Z",
          "iopub.execute_input": "2024-06-13T13:23:07.481119Z",
          "iopub.status.idle": "2024-06-13T13:23:14.254535Z",
          "shell.execute_reply.started": "2024-06-13T13:23:07.481081Z",
          "shell.execute_reply": "2024-06-13T13:23:14.253314Z"
        },
        "trusted": true,
        "id": "fOvej95BxVOP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = WrapperClass(64, 16, 8, 0.3)\n",
        "a(\"/content/downloaded_video.mp4\", \"see\")"
      ],
      "metadata": {
        "id": "oNTKZwC8xVOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3e17a3-9079-46ca-f7b6-56ed961b8013"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Using CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1/1 [==============================] - 1s 981ms/step\n",
            "1/1 [==============================] - 0s 390ms/step\n",
            "[[  0.0013018      0.9987]] (26, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 620.9ms\n",
            "Speed: 6.1ms preprocess, 620.9ms inference, 29.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 561.53149, 'y1': 171.50504, 'x2': 758.02905, 'y2': 477.70853}), ('Man', {'x1': 228.54907, 'y1': 96.98566, 'x2': 998.32617, 'y2': 717.63141}), ('Chair', {'x1': 798.90979, 'y1': 432.22934, 'x2': 1139.91052, 'y2': 719.86267}), ('Houseplant', {'x1': 1095.89551, 'y1': 325.46219, 'x2': 1280.0, 'y2': 719.4231}), ('Microphone', {'x1': 234.26419, 'y1': 421.36841, 'x2': 495.71494, 'y2': 639.00708}), ('Clothing', {'x1': 241.07996, 'y1': 407.42761, 'x2': 993.14587, 'y2': 717.78442})]\n",
            "30\n",
            "1/1 [==============================] - 0s 239ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[  0.0067195     0.99328]] (22, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 412.9ms\n",
            "Speed: 5.1ms preprocess, 412.9ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 535.04175, 'y1': 163.64868, 'x2': 728.52319, 'y2': 460.62744}), ('Man', {'x1': 236.29211, 'y1': 94.12842, 'x2': 980.37, 'y2': 715.79871}), ('Chair', {'x1': 804.26721, 'y1': 432.41754, 'x2': 1140.04797, 'y2': 719.72858}), ('Houseplant', {'x1': 1092.98657, 'y1': 325.69589, 'x2': 1280.0, 'y2': 719.29578}), ('Microphone', {'x1': 234.52881, 'y1': 420.89673, 'x2': 496.28345, 'y2': 637.54236}), ('Clothing', {'x1': 252.41867, 'y1': 405.21362, 'x2': 979.51562, 'y2': 717.15845})]\n",
            "60\n",
            "1/1 [==============================] - 0s 254ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[    0.00688     0.99312]] (22, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 394.7ms\n",
            "Speed: 5.3ms preprocess, 394.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 526.58905, 'y1': 166.38287, 'x2': 718.33368, 'y2': 462.04456}), ('Man', {'x1': 235.99164, 'y1': 97.69513, 'x2': 980.97333, 'y2': 715.59998}), ('Chair', {'x1': 802.2666, 'y1': 432.51086, 'x2': 1140.125, 'y2': 719.76062}), ('Houseplant', {'x1': 1094.06543, 'y1': 325.55664, 'x2': 1280.0, 'y2': 719.25732}), ('Microphone', {'x1': 237.35394, 'y1': 421.25174, 'x2': 494.40283, 'y2': 638.30005}), ('Clothing', {'x1': 255.80072, 'y1': 408.17911, 'x2': 980.76801, 'y2': 716.86365})]\n",
            "90\n",
            "None None []\n",
            "\n",
            "0: 384x640 1 Car, 398.3ms\n",
            "Speed: 4.6ms preprocess, 398.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Car', {'x1': 81.93066, 'y1': 269.72754, 'x2': 480.40479, 'y2': 551.6521})]\n",
            "120\n",
            "None None []\n",
            "\n",
            "0: 384x640 3 Cars, 395.4ms\n",
            "Speed: 3.0ms preprocess, 395.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Car', {'x1': 857.38721, 'y1': 224.97983, 'x2': 1193.61816, 'y2': 476.01389}), ('Car', {'x1': 76.62994, 'y1': 225.37123, 'x2': 455.44855, 'y2': 670.745}), ('Car', {'x1': 321.7218, 'y1': 225.5724, 'x2': 539.25696, 'y2': 411.67529})]\n",
            "150\n",
            "1/1 [==============================] - 0s 246ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "[[    0.25345     0.74655]] (26, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Man, 1 Microphone, 1 Person, 396.0ms\n",
            "Speed: 4.3ms preprocess, 396.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 529.20837, 'y1': 184.83543, 'x2': 727.3197, 'y2': 480.18909}), ('Man', {'x1': 241.81873, 'y1': 105.46722, 'x2': 964.43091, 'y2': 717.38147}), ('Person', {'x1': 223.021, 'y1': 105.29761, 'x2': 967.14563, 'y2': 718.35083}), ('Clothing', {'x1': 254.71271, 'y1': 416.41922, 'x2': 963.88751, 'y2': 717.62109}), ('Microphone', {'x1': 238.1142, 'y1': 421.9223, 'x2': 496.3537, 'y2': 639.92511})]\n",
            "180\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[   0.023818     0.97618]] (26, 32) ['Sad']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 921.3ms\n",
            "Speed: 2.8ms preprocess, 921.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 535.11938, 'y1': 180.85004, 'x2': 732.08252, 'y2': 476.953}), ('Man', {'x1': 242.71033, 'y1': 111.88617, 'x2': 969.36804, 'y2': 717.05676}), ('Chair', {'x1': 807.27527, 'y1': 431.78574, 'x2': 1139.1925, 'y2': 719.63977}), ('Houseplant', {'x1': 1097.57251, 'y1': 324.32214, 'x2': 1280.0, 'y2': 719.46606}), ('Clothing', {'x1': 250.21863, 'y1': 408.45181, 'x2': 968.87842, 'y2': 718.72083})]\n",
            "210\n",
            "1/1 [==============================] - 0s 433ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "[[     0.0134      0.9866]] (27, 33) ['Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Man, 1 Microphone, 604.6ms\n",
            "Speed: 2.4ms preprocess, 604.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 540.633, 'y1': 174.10007, 'x2': 734.19562, 'y2': 473.59024}), ('Man', {'x1': 248.20108, 'y1': 109.39081, 'x2': 936.79236, 'y2': 715.41882}), ('Clothing', {'x1': 263.06934, 'y1': 409.42166, 'x2': 924.99512, 'y2': 716.72656}), ('Microphone', {'x1': 235.47639, 'y1': 421.09351, 'x2': 497.36395, 'y2': 639.39331})]\n",
            "240\n",
            "1/1 [==============================] - 0s 460ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "[[   0.040341     0.95966]] (25, 32) ['Sad']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Man, 2 Microphones, 1 Person, 399.1ms\n",
            "Speed: 3.8ms preprocess, 399.1ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 539.85852, 'y1': 179.65353, 'x2': 737.9679, 'y2': 478.49808}), ('Microphone', {'x1': 232.89903, 'y1': 421.53644, 'x2': 495.29138, 'y2': 640.5163}), ('Man', {'x1': 223.2804, 'y1': 104.35815, 'x2': 943.13721, 'y2': 714.99622}), ('Clothing', {'x1': 261.0647, 'y1': 411.5426, 'x2': 930.51501, 'y2': 716.62793}), ('Person', {'x1': 77.20184, 'y1': 101.12476, 'x2': 946.46063, 'y2': 717.35156}), ('Microphone', {'x1': 3.7363, 'y1': 311.47168, 'x2': 365.72614, 'y2': 489.43774})]\n",
            "270\n",
            "1/1 [==============================] - 0s 245ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[  0.0062391     0.99376]] (22, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 386.1ms\n",
            "Speed: 4.2ms preprocess, 386.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 550.07928, 'y1': 174.33875, 'x2': 738.85443, 'y2': 470.67078}), ('Man', {'x1': 225.67603, 'y1': 102.72253, 'x2': 992.93469, 'y2': 716.45337}), ('Chair', {'x1': 798.05347, 'y1': 431.81912, 'x2': 1139.66284, 'y2': 719.80493}), ('Houseplant', {'x1': 1096.6488, 'y1': 324.65335, 'x2': 1280.0, 'y2': 719.25317}), ('Clothing', {'x1': 240.81226, 'y1': 408.19421, 'x2': 991.3927, 'y2': 717.50745}), ('Microphone', {'x1': 231.50731, 'y1': 420.85623, 'x2': 496.04578, 'y2': 639.34644})]\n",
            "300\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[   0.023731     0.97627]] (29, 37) ['Sad']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 391.4ms\n",
            "Speed: 4.8ms preprocess, 391.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 542.85791, 'y1': 180.49127, 'x2': 740.75195, 'y2': 480.48688}), ('Man', {'x1': 206.82385, 'y1': 105.6333, 'x2': 995.83447, 'y2': 715.26135}), ('Chair', {'x1': 806.2666, 'y1': 431.57428, 'x2': 1139.33691, 'y2': 719.97235}), ('Houseplant', {'x1': 1097.29639, 'y1': 325.49176, 'x2': 1280.0, 'y2': 719.09808}), ('Clothing', {'x1': 225.81705, 'y1': 407.67099, 'x2': 994.36267, 'y2': 717.07068})]\n",
            "330\n",
            "1/1 [==============================] - 0s 237ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[   0.025328     0.97467]] (29, 37) ['Sad']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 391.5ms\n",
            "Speed: 5.8ms preprocess, 391.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 536.89282, 'y1': 184.189, 'x2': 736.2124, 'y2': 482.28201}), ('Man', {'x1': 218.26645, 'y1': 105.87439, 'x2': 997.85535, 'y2': 715.48547}), ('Chair', {'x1': 814.34192, 'y1': 432.24426, 'x2': 1139.81006, 'y2': 719.88318}), ('Houseplant', {'x1': 1098.21423, 'y1': 323.83472, 'x2': 1280.0, 'y2': 719.22998}), ('Clothing', {'x1': 246.50537, 'y1': 406.88901, 'x2': 996.14746, 'y2': 716.97131}), ('Microphone', {'x1': 234.54529, 'y1': 421.30115, 'x2': 496.35449, 'y2': 636.79663})]\n",
            "360\n",
            "1/1 [==============================] - 0s 239ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[  0.0060953      0.9939]] (26, 33) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Houseplant, 1 Human face, 1 Man, 398.1ms\n",
            "Speed: 2.6ms preprocess, 398.1ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 550.9071, 'y1': 189.92889, 'x2': 748.72998, 'y2': 492.26172}), ('Man', {'x1': 109.05762, 'y1': 102.90277, 'x2': 984.24121, 'y2': 717.06451}), ('Chair', {'x1': 808.55347, 'y1': 432.36575, 'x2': 1140.0625, 'y2': 719.91357}), ('Houseplant', {'x1': 1098.52844, 'y1': 323.79361, 'x2': 1280.0, 'y2': 719.29211})]\n",
            "390\n",
            "1/1 [==============================] - 0s 239ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[  0.0052839     0.99472]] (29, 33) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 393.4ms\n",
            "Speed: 2.5ms preprocess, 393.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 528.12305, 'y1': 164.29205, 'x2': 718.24866, 'y2': 453.10455}), ('Man', {'x1': 242.65155, 'y1': 98.6012, 'x2': 1005.15643, 'y2': 716.20825}), ('Chair', {'x1': 814.14893, 'y1': 432.36108, 'x2': 1139.57483, 'y2': 719.91162}), ('Houseplant', {'x1': 1097.71411, 'y1': 324.06622, 'x2': 1280.0, 'y2': 719.19745}), ('Microphone', {'x1': 237.42999, 'y1': 421.50122, 'x2': 496.27795, 'y2': 638.39636}), ('Clothing', {'x1': 252.48743, 'y1': 405.98303, 'x2': 1004.31006, 'y2': 717.60901})]\n",
            "420\n",
            "1/1 [==============================] - 0s 241ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[  0.0049428     0.99506]] (26, 35) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 395.6ms\n",
            "Speed: 4.5ms preprocess, 395.6ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Man', {'x1': 208.94308, 'y1': 94.77161, 'x2': 1002.76904, 'y2': 715.23425}), ('Human face', {'x1': 557.01782, 'y1': 166.55075, 'x2': 754.85938, 'y2': 467.91965}), ('Chair', {'x1': 809.23975, 'y1': 432.60492, 'x2': 1140.9585, 'y2': 719.97614}), ('Houseplant', {'x1': 1098.49463, 'y1': 323.88531, 'x2': 1280.0, 'y2': 719.19574}), ('Clothing', {'x1': 240.81787, 'y1': 412.75256, 'x2': 1001.61475, 'y2': 716.99109}), ('Microphone', {'x1': 231.32642, 'y1': 421.10736, 'x2': 495.93274, 'y2': 639.2254})]\n",
            "450\n",
            "None None []\n",
            "\n",
            "0: 384x640 1 Computer monitor, 1 Tablet computer, 620.8ms\n",
            "Speed: 2.5ms preprocess, 620.8ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Computer monitor', {'x1': 36.91772, 'y1': 28.8222, 'x2': 1228.13965, 'y2': 701.953}), ('Tablet computer', {'x1': 26.43079, 'y1': 28.32697, 'x2': 1237.55127, 'y2': 701.04596})]\n",
            "480\n",
            "1/1 [==============================] - 0s 429ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "[[       0.33        0.67]] (35, 43) ['Surprise']\n",
            "\n",
            "0: 384x640 1 Computer monitor, 576.8ms\n",
            "Speed: 4.9ms preprocess, 576.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Computer monitor', {'x1': 10.49792, 'y1': 33.71936, 'x2': 1225.63574, 'y2': 699.35571})]\n",
            "510\n",
            "None None []\n",
            "\n",
            "0: 384x640 1 Computer monitor, 391.9ms\n",
            "Speed: 4.5ms preprocess, 391.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Computer monitor', {'x1': 15.93909, 'y1': 29.91199, 'x2': 1234.74182, 'y2': 695.5824})]\n",
            "540\n",
            "None None []\n",
            "\n",
            "0: 384x640 1 Tablet computer, 388.8ms\n",
            "Speed: 3.6ms preprocess, 388.8ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Tablet computer', {'x1': 45.46997, 'y1': 22.03412, 'x2': 1247.11621, 'y2': 697.74841})]\n",
            "570\n",
            "1/1 [==============================] - 0s 236ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "[[  0.0073046      0.9927]] (21, 29) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 2 Microphones, 405.0ms\n",
            "Speed: 4.6ms preprocess, 405.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Chair', {'x1': 810.08228, 'y1': 432.48065, 'x2': 1140.00244, 'y2': 719.87909}), ('Human face', {'x1': 504.80798, 'y1': 162.14955, 'x2': 699.06165, 'y2': 450.51672}), ('Man', {'x1': 224.25787, 'y1': 99.55237, 'x2': 998.1767, 'y2': 715.18665}), ('Houseplant', {'x1': 1094.58813, 'y1': 324.19962, 'x2': 1280.0, 'y2': 719.09045}), ('Microphone', {'x1': 237.71892, 'y1': 421.27737, 'x2': 496.8819, 'y2': 637.13062}), ('Clothing', {'x1': 256.62964, 'y1': 405.60443, 'x2': 995.05615, 'y2': 717.34015}), ('Microphone', {'x1': 178.67438, 'y1': 317.18268, 'x2': 364.06207, 'y2': 495.11279})]\n",
            "600\n",
            "1/1 [==============================] - 0s 246ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[  0.0089978       0.991]] (22, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 2 Microphones, 393.8ms\n",
            "Speed: 2.4ms preprocess, 393.8ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 534.40588, 'y1': 168.68951, 'x2': 730.10803, 'y2': 472.33954}), ('Man', {'x1': 16.13568, 'y1': 90.27441, 'x2': 969.68585, 'y2': 716.02234}), ('Chair', {'x1': 813.24316, 'y1': 433.06638, 'x2': 1140.77576, 'y2': 719.70093}), ('Houseplant', {'x1': 1095.64551, 'y1': 324.01633, 'x2': 1280.0, 'y2': 719.16357}), ('Clothing', {'x1': 247.78125, 'y1': 404.22455, 'x2': 968.39246, 'y2': 716.63629}), ('Microphone', {'x1': 235.25439, 'y1': 421.44238, 'x2': 496.25037, 'y2': 637.08997}), ('Microphone', {'x1': 177.19519, 'y1': 316.90491, 'x2': 364.49591, 'y2': 496.08716})]\n",
            "630\n",
            "1/1 [==============================] - 0s 238ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[  0.0057527     0.99425]] (22, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 2 Microphones, 415.8ms\n",
            "Speed: 2.6ms preprocess, 415.8ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 522.26514, 'y1': 166.18616, 'x2': 714.13501, 'y2': 460.92584}), ('Chair', {'x1': 810.06226, 'y1': 432.33301, 'x2': 1140.3269, 'y2': 719.88904}), ('Man', {'x1': 233.52039, 'y1': 91.68903, 'x2': 995.6427, 'y2': 715.38629}), ('Houseplant', {'x1': 1094.36719, 'y1': 324.16199, 'x2': 1280.0, 'y2': 719.10168}), ('Microphone', {'x1': 237.7291, 'y1': 421.67786, 'x2': 496.25443, 'y2': 637.83447}), ('Clothing', {'x1': 248.81238, 'y1': 404.6077, 'x2': 993.71216, 'y2': 717.3186}), ('Microphone', {'x1': 177.33032, 'y1': 317.87396, 'x2': 364.22614, 'y2': 494.24323})]\n",
            "660\n",
            "1/1 [==============================] - 0s 251ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[   0.004336     0.99566]] (29, 37) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 2 Microphones, 392.9ms\n",
            "Speed: 3.1ms preprocess, 392.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 541.25842, 'y1': 167.13855, 'x2': 734.7782, 'y2': 462.03406}), ('Man', {'x1': 12.58118, 'y1': 94.7467, 'x2': 988.21002, 'y2': 717.47852}), ('Houseplant', {'x1': 1102.03931, 'y1': 327.03937, 'x2': 1279.88306, 'y2': 719.38654}), ('Clothing', {'x1': 245.70917, 'y1': 405.46481, 'x2': 999.96979, 'y2': 718.25684}), ('Chair', {'x1': 810.03821, 'y1': 432.37576, 'x2': 1141.88367, 'y2': 716.40576}), ('Microphone', {'x1': 233.99471, 'y1': 421.09323, 'x2': 496.22882, 'y2': 634.90698}), ('Microphone', {'x1': 3.83118, 'y1': 314.88763, 'x2': 372.70343, 'y2': 489.13959})]\n",
            "690\n",
            "1/1 [==============================] - 0s 237ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[  0.0067785     0.99322]] (29, 37) ['Happy']\n",
            "\n",
            "0: 384x640 2 Chairs, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1485.2ms\n",
            "Speed: 3.3ms preprocess, 1485.2ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Chair', {'x1': 812.18494, 'y1': 432.33264, 'x2': 1139.33875, 'y2': 719.75012}), ('Human face', {'x1': 543.26208, 'y1': 174.5524, 'x2': 736.96448, 'y2': 478.85367}), ('Man', {'x1': 125.26538, 'y1': 94.2348, 'x2': 988.28809, 'y2': 716.88232}), ('Chair', {'x1': 910.63849, 'y1': 99.42565, 'x2': 1191.06274, 'y2': 350.01447}), ('Houseplant', {'x1': 1102.93921, 'y1': 322.84814, 'x2': 1279.85913, 'y2': 719.09009}), ('Clothing', {'x1': 243.72873, 'y1': 405.32468, 'x2': 994.33691, 'y2': 717.14722})]\n",
            "720\n",
            "1/1 [==============================] - 0s 238ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[   0.016334     0.98367]] (28, 33) ['Happy']\n",
            "\n",
            "0: 384x640 2 Chairs, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 388.9ms\n",
            "Speed: 2.7ms preprocess, 388.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Chair', {'x1': 807.19446, 'y1': 431.93674, 'x2': 1138.97839, 'y2': 720.0}), ('Human face', {'x1': 559.96796, 'y1': 180.43655, 'x2': 757.58966, 'y2': 481.7247}), ('Man', {'x1': 79.25464, 'y1': 94.76538, 'x2': 1007.42126, 'y2': 716.70679}), ('Houseplant', {'x1': 1103.21436, 'y1': 327.12289, 'x2': 1279.8689, 'y2': 719.09656}), ('Chair', {'x1': 904.77612, 'y1': 90.32776, 'x2': 1202.55811, 'y2': 348.81989}), ('Clothing', {'x1': 245.45618, 'y1': 409.4071, 'x2': 1012.95508, 'y2': 717.67407}), ('Microphone', {'x1': 178.32739, 'y1': 316.58984, 'x2': 362.91919, 'y2': 492.69409})]\n",
            "750\n",
            "1/1 [==============================] - 0s 448ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "[[   0.020623     0.97938]\n",
            " [    0.25742     0.74258]] (25, 32) ['Happy', 'Sad']\n",
            "\n",
            "0: 384x640 1 Chair, 2 Clothings, 1 Houseplant, 2 Human faces, 2 Mans, 636.8ms\n",
            "Speed: 7.5ms preprocess, 636.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 1028.10767, 'y1': 101.55435, 'x2': 1125.9729, 'y2': 244.77856}), ('Man', {'x1': 870.9978, 'y1': 66.26532, 'x2': 1240.32007, 'y2': 352.62448}), ('Human face', {'x1': 508.40393, 'y1': 167.1955, 'x2': 703.15283, 'y2': 457.80731}), ('Man', {'x1': 136.24377, 'y1': 97.13196, 'x2': 994.56055, 'y2': 716.97626}), ('Houseplant', {'x1': 1103.60132, 'y1': 337.61462, 'x2': 1279.98755, 'y2': 718.61438}), ('Clothing', {'x1': 250.9603, 'y1': 392.00769, 'x2': 989.78748, 'y2': 715.63708}), ('Clothing', {'x1': 870.70776, 'y1': 226.63315, 'x2': 1239.7207, 'y2': 348.89621}), ('Chair', {'x1': 815.64185, 'y1': 431.17065, 'x2': 1142.12988, 'y2': 716.1344})]\n",
            "780\n",
            "1/1 [==============================] - 0s 454ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[[   0.020257     0.97974]\n",
            " [    0.30146     0.69854]] (26, 33) ['Happy', 'Sad']\n",
            "\n",
            "0: 384x640 1 Chair, 2 Clothings, 1 Houseplant, 2 Human faces, 2 Mans, 2 Microphones, 398.0ms\n",
            "Speed: 5.2ms preprocess, 398.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 988.15881, 'y1': 106.63223, 'x2': 1087.35339, 'y2': 250.88385}), ('Human face', {'x1': 546.37219, 'y1': 176.71497, 'x2': 744.82166, 'y2': 485.18469}), ('Man', {'x1': 884.73083, 'y1': 68.78708, 'x2': 1220.14246, 'y2': 350.56213}), ('Man', {'x1': 231.2561, 'y1': 95.64856, 'x2': 1019.75659, 'y2': 716.85486}), ('Houseplant', {'x1': 1102.99072, 'y1': 335.26492, 'x2': 1280.0, 'y2': 718.55566}), ('Clothing', {'x1': 880.43921, 'y1': 229.23705, 'x2': 1220.08521, 'y2': 347.67236}), ('Clothing', {'x1': 252.8811, 'y1': 410.63141, 'x2': 1017.34021, 'y2': 717.40094}), ('Microphone', {'x1': 238.4478, 'y1': 421.22388, 'x2': 495.31525, 'y2': 637.60388}), ('Microphone', {'x1': 178.88304, 'y1': 316.81171, 'x2': 361.64124, 'y2': 490.69342}), ('Chair', {'x1': 810.87048, 'y1': 431.42072, 'x2': 1142.23511, 'y2': 717.45892})]\n",
            "810\n",
            "1/1 [==============================] - 0s 238ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[   0.010986     0.98901]] (28, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 395.9ms\n",
            "Speed: 4.7ms preprocess, 395.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 518.36853, 'y1': 164.89705, 'x2': 706.64465, 'y2': 452.63165}), ('Man', {'x1': 181.29587, 'y1': 96.13989, 'x2': 1025.03442, 'y2': 717.43604}), ('Houseplant', {'x1': 1096.60315, 'y1': 324.10434, 'x2': 1280.0, 'y2': 719.24695}), ('Clothing', {'x1': 245.36493, 'y1': 409.05078, 'x2': 949.5921, 'y2': 717.1604})]\n",
            "840\n",
            "1/1 [==============================] - 0s 243ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[  0.0067069     0.99329]] (29, 37) ['Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Man, 1 Microphone, 407.4ms\n",
            "Speed: 5.3ms preprocess, 407.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 525.83911, 'y1': 170.96246, 'x2': 720.56567, 'y2': 459.20203}), ('Man', {'x1': 222.47128, 'y1': 97.52319, 'x2': 981.28894, 'y2': 717.17065}), ('Microphone', {'x1': 240.71671, 'y1': 421.06833, 'x2': 494.50705, 'y2': 635.59155}), ('Clothing', {'x1': 254.34216, 'y1': 412.08112, 'x2': 974.65552, 'y2': 717.71844})]\n",
            "870\n",
            "1/1 [==============================] - 0s 233ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[  0.0052578     0.99474]] (29, 35) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 401.7ms\n",
            "Speed: 2.6ms preprocess, 401.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 523.13208, 'y1': 170.78659, 'x2': 711.74097, 'y2': 463.25003}), ('Chair', {'x1': 804.67151, 'y1': 432.52835, 'x2': 1139.97131, 'y2': 719.77783}), ('Man', {'x1': 225.7663, 'y1': 102.16583, 'x2': 985.95795, 'y2': 714.98206}), ('Houseplant', {'x1': 1099.86328, 'y1': 325.53998, 'x2': 1280.0, 'y2': 719.33551}), ('Microphone', {'x1': 235.90247, 'y1': 421.12634, 'x2': 495.1579, 'y2': 646.47839}), ('Clothing', {'x1': 316.17477, 'y1': 406.73807, 'x2': 982.88354, 'y2': 717.24463})]\n",
            "900\n",
            "1/1 [==============================] - 0s 421ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "[[  0.0059423     0.99406]] (29, 35) ['Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Man, 1 Microphone, 608.6ms\n",
            "Speed: 2.4ms preprocess, 608.6ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 532.4953, 'y1': 170.83275, 'x2': 724.92352, 'y2': 465.20117}), ('Man', {'x1': 239.60043, 'y1': 97.75232, 'x2': 1003.46252, 'y2': 717.28406}), ('Microphone', {'x1': 239.2101, 'y1': 421.37695, 'x2': 494.27454, 'y2': 637.40027}), ('Clothing', {'x1': 275.3324, 'y1': 409.47458, 'x2': 1006.63281, 'y2': 717.83423})]\n",
            "930\n",
            "1/1 [==============================] - 0s 465ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "[[   0.087555     0.91244]] (26, 32) ['Sad']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 577.6ms\n",
            "Speed: 2.7ms preprocess, 577.6ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 515.35217, 'y1': 173.69463, 'x2': 715.19446, 'y2': 460.98627}), ('Man', {'x1': 211.98523, 'y1': 105.88763, 'x2': 995.07019, 'y2': 716.20105}), ('Chair', {'x1': 806.55334, 'y1': 432.71417, 'x2': 1139.75195, 'y2': 719.87592}), ('Houseplant', {'x1': 1099.48901, 'y1': 324.85687, 'x2': 1280.0, 'y2': 719.19855}), ('Clothing', {'x1': 307.08542, 'y1': 404.69135, 'x2': 991.82373, 'y2': 717.54523}), ('Microphone', {'x1': 315.75867, 'y1': 421.97815, 'x2': 495.07043, 'y2': 574.49146})]\n",
            "960\n",
            "1/1 [==============================] - 0s 268ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[  0.0029568     0.99704]] (26, 33) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 394.9ms\n",
            "Speed: 2.4ms preprocess, 394.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Chair', {'x1': 802.43359, 'y1': 431.99771, 'x2': 1140.21387, 'y2': 720.0}), ('Human face', {'x1': 552.30383, 'y1': 167.80154, 'x2': 746.54919, 'y2': 464.56027}), ('Houseplant', {'x1': 1099.19385, 'y1': 326.24158, 'x2': 1280.0, 'y2': 719.53638}), ('Man', {'x1': 240.67139, 'y1': 91.73193, 'x2': 1008.82556, 'y2': 715.05872}), ('Microphone', {'x1': 3.63315, 'y1': 315.14087, 'x2': 374.45731, 'y2': 493.12982}), ('Clothing', {'x1': 323.45285, 'y1': 407.43292, 'x2': 1005.85596, 'y2': 716.7851})]\n",
            "990\n",
            "1/1 [==============================] - 0s 239ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[   0.010583     0.98942]] (22, 29) ['Sad']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Person, 399.7ms\n",
            "Speed: 3.4ms preprocess, 399.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Person', {'x1': 815.43298, 'y1': 443.40906, 'x2': 1153.91223, 'y2': 694.11377}), ('Human face', {'x1': 980.1272, 'y1': 480.76199, 'x2': 1061.245, 'y2': 597.48511}), ('Clothing', {'x1': 845.96301, 'y1': 564.58008, 'x2': 1151.83875, 'y2': 692.33984})]\n",
            "1020\n",
            "1/1 [==============================] - 0s 244ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[   0.034056     0.96594]] (26, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Man, 1 Person, 447.4ms\n",
            "Speed: 2.6ms preprocess, 447.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 955.71472, 'y1': 474.03113, 'x2': 1034.28748, 'y2': 588.99048}), ('Person', {'x1': 791.89954, 'y1': 440.75616, 'x2': 1143.48071, 'y2': 698.83771}), ('Man', {'x1': 790.25183, 'y1': 442.47797, 'x2': 1143.44214, 'y2': 700.23895})]\n",
            "1050\n",
            "1/1 [==============================] - 0s 250ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "[[   0.048249     0.95175]] (28, 33) ['Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Person, 399.7ms\n",
            "Speed: 4.7ms preprocess, 399.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 971.60596, 'y1': 475.01428, 'x2': 1050.42261, 'y2': 596.11926}), ('Person', {'x1': 794.8739, 'y1': 439.66022, 'x2': 1150.07666, 'y2': 695.03033}), ('Clothing', {'x1': 845.89307, 'y1': 561.42456, 'x2': 1151.32251, 'y2': 692.68164})]\n",
            "1080\n",
            "1/1 [==============================] - 0s 233ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[   0.016061     0.98394]] (29, 33) ['Sad']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 408.0ms\n",
            "Speed: 6.1ms preprocess, 408.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Person', {'x1': 809.82922, 'y1': 445.26727, 'x2': 1146.52356, 'y2': 695.31757}), ('Human face', {'x1': 970.60632, 'y1': 485.50519, 'x2': 1049.50256, 'y2': 604.17084})]\n",
            "1110\n",
            "1/1 [==============================] - 0s 248ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[[   0.012035     0.98797]] (26, 33) ['Happy']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 396.4ms\n",
            "Speed: 4.3ms preprocess, 396.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Person', {'x1': 812.18103, 'y1': 445.58844, 'x2': 1146.02832, 'y2': 695.26215}), ('Human face', {'x1': 967.69275, 'y1': 483.73254, 'x2': 1046.46204, 'y2': 599.82947})]\n",
            "1140\n",
            "1/1 [==============================] - 1s 797ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "[[   0.062445     0.93755]\n",
            " [    0.19571     0.80429]] (29, 37) ['Sad', 'Surprise']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 642.4ms\n",
            "Speed: 4.6ms preprocess, 642.4ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 967.54297, 'y1': 476.03891, 'x2': 1046.44019, 'y2': 596.03186}), ('Person', {'x1': 818.66113, 'y1': 441.33453, 'x2': 1147.51477, 'y2': 695.84369})]\n",
            "1170\n",
            "1/1 [==============================] - 1s 892ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[   0.040737     0.95926]\n",
            " [    0.27401     0.72599]\n",
            " [    0.14056     0.85943]] (26, 32) ['Happy', 'Surprise', 'Surprise']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 394.6ms\n",
            "Speed: 5.8ms preprocess, 394.6ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 957.46558, 'y1': 476.85181, 'x2': 1033.56677, 'y2': 593.75134}), ('Person', {'x1': 827.28442, 'y1': 442.26465, 'x2': 1137.59277, 'y2': 695.40149})]\n",
            "1200\n",
            "1/1 [==============================] - 1s 654ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[    0.17588     0.82412]\n",
            " [    0.21222     0.78778]\n",
            " [    0.18458     0.81542]] (26, 33) ['Sad', 'Happy', 'Surprise']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Person, 401.1ms\n",
            "Speed: 6.6ms preprocess, 401.1ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Person', {'x1': 829.31787, 'y1': 441.80103, 'x2': 1143.27661, 'y2': 694.92004}), ('Human face', {'x1': 961.30237, 'y1': 477.59329, 'x2': 1040.04431, 'y2': 596.9585}), ('Clothing', {'x1': 849.65454, 'y1': 562.35291, 'x2': 1145.64258, 'y2': 691.75061})]\n",
            "1230\n",
            "1/1 [==============================] - 0s 457ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[    0.05371     0.94629]\n",
            " [    0.19958     0.80042]] (26, 33) ['Happy', 'Happy']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Man, 1 Person, 395.3ms\n",
            "Speed: 4.0ms preprocess, 395.3ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 967.85034, 'y1': 476.26074, 'x2': 1046.28418, 'y2': 596.17761}), ('Person', {'x1': 824.22791, 'y1': 442.01962, 'x2': 1143.74109, 'y2': 692.7052}), ('Man', {'x1': 824.27051, 'y1': 445.78156, 'x2': 1143.2749, 'y2': 692.81207})]\n",
            "1260\n",
            "1/1 [==============================] - 0s 443ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[   0.021746     0.97825]\n",
            " [    0.19999     0.80001]] (29, 35) ['Fear', 'Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Person, 410.7ms\n",
            "Speed: 2.5ms preprocess, 410.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Person', {'x1': 826.94116, 'y1': 442.17542, 'x2': 1144.12402, 'y2': 693.8158}), ('Human face', {'x1': 962.06787, 'y1': 477.33093, 'x2': 1039.51367, 'y2': 595.302}), ('Clothing', {'x1': 848.40271, 'y1': 556.62036, 'x2': 1144.80969, 'y2': 691.5459})]\n",
            "1290\n",
            "1/1 [==============================] - 0s 443ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[   0.027285     0.97271]\n",
            " [    0.22912     0.77088]] (29, 35) ['Happy', 'Happy']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 407.4ms\n",
            "Speed: 3.0ms preprocess, 407.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Person', {'x1': 820.46326, 'y1': 442.92627, 'x2': 1145.16626, 'y2': 694.52576}), ('Human face', {'x1': 963.10803, 'y1': 477.63867, 'x2': 1042.10999, 'y2': 596.37915})]\n",
            "1320\n",
            "1/1 [==============================] - 0s 471ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "[[   0.064119     0.93588]\n",
            " [    0.20426     0.79574]] (29, 33) ['Happy', 'Happy']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 647.1ms\n",
            "Speed: 3.9ms preprocess, 647.1ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Person', {'x1': 821.40027, 'y1': 443.06171, 'x2': 1138.14709, 'y2': 693.94904}), ('Human face', {'x1': 964.27808, 'y1': 480.90515, 'x2': 1041.66919, 'y2': 597.88708})]\n",
            "1350\n",
            "1/1 [==============================] - 1s 765ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "[[   0.041476     0.95852]\n",
            " [    0.20426     0.79574]] (20, 29) ['Happy', 'Happy']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 695.2ms\n",
            "Speed: 2.5ms preprocess, 695.2ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 963.82715, 'y1': 479.78021, 'x2': 1040.78076, 'y2': 598.83441}), ('Person', {'x1': 821.56787, 'y1': 442.65979, 'x2': 1140.00977, 'y2': 694.18958})]\n",
            "1380\n",
            "1/1 [==============================] - 0s 453ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[   0.015926     0.98407]\n",
            " [    0.20426     0.79574]] (16, 22) ['Surprise', 'Happy']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 398.1ms\n",
            "Speed: 2.6ms preprocess, 398.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 967.78943, 'y1': 475.41068, 'x2': 1043.29761, 'y2': 597.14087}), ('Person', {'x1': 822.39087, 'y1': 441.38696, 'x2': 1141.79626, 'y2': 695.6488})]\n",
            "1410\n",
            "1/1 [==============================] - 0s 434ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "[[   0.059968     0.94003]\n",
            " [    0.20426     0.79574]] (24, 30) ['Happy', 'Happy']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 417.0ms\n",
            "Speed: 4.9ms preprocess, 417.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 955.87488, 'y1': 477.55121, 'x2': 1034.36169, 'y2': 594.86591}), ('Person', {'x1': 821.84546, 'y1': 442.9184, 'x2': 1139.12646, 'y2': 694.06586})]\n",
            "1440\n",
            "1/1 [==============================] - 0s 443ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[   0.076276     0.92372]\n",
            " [    0.19509     0.80491]] (20, 29) ['Happy', 'Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Person, 429.4ms\n",
            "Speed: 5.9ms preprocess, 429.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Person', {'x1': 807.00635, 'y1': 442.53009, 'x2': 1143.89648, 'y2': 695.07306}), ('Human face', {'x1': 959.27051, 'y1': 474.73761, 'x2': 1036.77344, 'y2': 588.26617}), ('Clothing', {'x1': 850.09265, 'y1': 562.19122, 'x2': 1146.36011, 'y2': 691.49213})]\n",
            "1470\n",
            "1/1 [==============================] - 0s 441ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "[[   0.078847     0.92115]\n",
            " [    0.19519     0.80481]] (29, 33) ['Sad', 'Happy']\n",
            "\n",
            "0: 384x640 1 Human face, 1 Person, 423.5ms\n",
            "Speed: 2.6ms preprocess, 423.5ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 963.11987, 'y1': 481.64301, 'x2': 1043.59863, 'y2': 600.65155}), ('Person', {'x1': 832.146, 'y1': 443.61737, 'x2': 1151.24683, 'y2': 697.01276})]\n",
            "96.05267763137817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a(\"/content/downloaded_video.mp4\", \"ocr\")"
      ],
      "metadata": {
        "id": "DRFkgNUyUkQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14bcad9-f1ec-43b2-a3ba-2f70d2a72eeb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "30\n",
            "\n",
            "60\n",
            "\n",
            "90\n",
            "Car 0.941 cc7_0 car 0.69; Car 0.90 UG\n",
            "120\n",
            "Cat 0 89 car 0.81ar Ocar 0 icar_0.94\n",
            "150\n",
            "71aaeae Ultrollytics YOLOv8 Ton\n",
            "180\n",
            "NEW TOHORROw is a new tohorrow.\n",
            "210\n",
            "Ultralytics ShAPI.\n",
            "240\n",
            "The new technology is ultralytical.\n",
            "270\n",
            "Is it possible?\n",
            "300\n",
            "Is it possible?\n",
            "330\n",
            "New to the forecast, the forecast is for the future.\n",
            "360\n",
            "IoRrow.\n",
            "390\n",
            "SHAPIMG L Inaney is a SHAPIMG L Inaney\n",
            "420\n",
            "New TQVOrROw is announcing a new TQVOrRO\n",
            "450\n",
            "YOLOv8 is a language designed to be fast, accurate, and easy\n",
            "480\n",
            "To request an Enterprise License, please browse the YOLOv8 Docs for details\n",
            "510\n",
            "Use CLI YOLOv8 may be used directly in the Command Line Interface (\n",
            "540\n",
            "For making predictions using trained YOLOv8 model on custom datasets, the following\n",
            "570\n",
            "IcnoRROw is a shaminc mew Ic\n",
            "600\n",
            "Fasie Shaping New Tomorrow!\n",
            "630\n",
            "Shading New Chorrow, the new town of Chorrow.\n",
            "660\n",
            "YolovR Inletenc \"FPS: 67 person 0.90 JF'\n",
            "690\n",
            "86 pottod elont 044 chair 0.69_ c0l\n",
            "720\n",
            "92 potted_plont 0.69 chair 0.75_ 'TOHOR\n",
            "750\n",
            "ILJClorg Infrteno FPEILZ person0.82 chain 0.\n",
            "780\n",
            "LJClors Infrtencr Fes:77 person 0.85 chair 04\n",
            "810\n",
            "Then, the next day, ShapiMG MewTeeteet\n",
            "840\n",
            "\n",
            "870\n",
            "\n",
            "900\n",
            "\n",
            "930\n",
            "\n",
            "960\n",
            "Tomorrow ShaPINO MEW - - - - - \n",
            "990\n",
            "The YOLOv8 is an excellent choice for wide range tracking, instance segmentation\n",
            "1020\n",
            "The YOLOv8 is an edge, state-of-the-art (\n",
            "1050\n",
            "The YOLOv8 is an edge, state-of-the-art (\n",
            "1080\n",
            "The YOLOv8 is a cutting-edge state-of-the-\n",
            "1110\n",
            "The YOLOv8 app is a cutting-edge state-of-the\n",
            "1140\n",
            "The resources here will help you get the most out of YOLOv8. Please\n",
            "1170\n",
            "The resources here will help you get the most out of YOLOv8. Please\n",
            "1200\n",
            "The resources here will help you get the most out of YOLOv8. Please\n",
            "1230\n",
            "The resources here will help you get the most out of YOLOv8! Please\n",
            "1260\n",
            "The resources here will help you get the most out of YOLOv8! Please\n",
            "1290\n",
            "The resources here will help you get the most out of YOLOv8. Please\n",
            "1320\n",
            "The resources here will help you get the most out of YOLOv8. Please\n",
            "1350\n",
            "The resources here will help you get the most out of YOLOv8. Please\n",
            "1380\n",
            "The resources here will help you get the most out of YOLOv8. Please\n",
            "1410\n",
            "The resources here will help you get the most out of YOLOv8. Please\n",
            "1440\n",
            "The resources here will help you get the most out of YOLOv8! Please\n",
            "1470\n",
            "The resources here will help you get the most out of YOLOv8! Please\n",
            "1086.7384552955627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PT52FGMBwUPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}