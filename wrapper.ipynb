{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 64424,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 53721
        },
        {
          "sourceId": 60178,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 50333
        }
      ],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MBUYt0n/blind-eye-dealers/blob/main/wrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'emogen2/tensorflow2/one/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F53721%2F64424%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240613%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240613T132419Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da274f4e22fc82aa42ef2bd6731c6d2033b1d6ecf9240ba2a39989384ef7f6a49300e2c38c10c4cb235078f88bd815acf60b8249e16ce719a95388786cfc98f5d97802a3cd7824d16ecdf6acec387a6d0b223959fda03c3a0e95e1f0dfea8fefc83e4db5de89a7dd1fb3b241c94b2c05f53d6546f4292342b19f9afdb98a4f524365e3a2a7a9b7dbec8c1030d23b5a436a406a9fef7029251fd1521670b2f364fd2634c412170fd85cb622978eed2ff3348ed258ddabfd95fd72984968aa01802b4557768f3c6868c7b72f5c53fa5a152e8a9fea3f55285f1d0715b254b38fb0a338b0a135635f4690d81724ce333ac67c203165224b401e17f9296c433be87bc,yolov8/pytorch/open-image-v8/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F50333%2F60178%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240613%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240613T132420Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1e6139bd454ddc39527c6e4062138f8b3b2ce1f8a454ae59f4952c82199e4cd67c811b5d4214568f25602af5657b683f1e7624683aa23c6324e4b48f3af49be303a8a6e11a4e64318078671019067e9154e3e1be9b615d3d3a0030953e66facbb9392332a34cc69bea67d7d6234da21d6ef240e8a779e787c3c1c6858593e238a2f24e667ded02600c6feffb78195034cc8bc2457a925790469787bf3d2299179624020d0344aad29315c5f9bab2cb9c2f860fb22c836c6ea9093f32f979eb7dd5420485c25b919581e693f496031c7b17c9acf3a08de1d02dcc353ef91c34510a6f872894767485aa0b0da00565f63aa90e2dd32eb5f3ede6a401c8ffb1b2a7'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "XIxFk-xVxVOM",
        "outputId": "7391920b-2d0e-44f1-808a-1ca210433a83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading emogen2/tensorflow2/one/1, 203010682 bytes compressed\n",
            "[==================================================] 203010682 bytes downloaded\n",
            "Downloaded and uncompressed: emogen2/tensorflow2/one/1\n",
            "Downloading yolov8/pytorch/open-image-v8/1, 21264265 bytes compressed\n",
            "[==================================================] 21264265 bytes downloaded\n",
            "Downloaded and uncompressed: yolov8/pytorch/open-image-v8/1\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics lapx paddleocr paddlepaddle pytube dlib"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-06-13T12:55:36.04466Z",
          "iopub.execute_input": "2024-06-13T12:55:36.045766Z",
          "iopub.status.idle": "2024-06-13T13:04:14.240029Z",
          "shell.execute_reply.started": "2024-06-13T12:55:36.045716Z",
          "shell.execute_reply": "2024-06-13T13:04:14.238418Z"
        },
        "trusted": true,
        "id": "vjg41KcUxVON",
        "outputId": "d13f4a30-a5e2-4e8b-a36b-7f86d3149a25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.32)\n",
            "Requirement already satisfied: lapx in /usr/local/lib/python3.10/dist-packages (0.5.9)\n",
            "Requirement already satisfied: paddleocr in /usr/local/lib/python3.10/dist-packages (2.7.3)\n",
            "Requirement already satisfied: paddlepaddle in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.10/dist-packages (15.0.0)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.6.0.66)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.0.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: ultralytics-thop>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.2.8)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from lapx) (1.25.2)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from paddleocr) (2.0.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.19.3)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.4.0)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.10/dist-packages (from paddleocr) (1.3.0.post5)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.10/dist-packages (from paddleocr) (1.4.1)\n",
            "Requirement already satisfied: visualdl in /usr/local/lib/python3.10/dist-packages (from paddleocr) (2.5.3)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from paddleocr) (3.9.3)\n",
            "Requirement already satisfied: opencv-contrib-python<=4.6.0.66 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.6.0.66)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from paddleocr) (3.0.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.9.4)\n",
            "Requirement already satisfied: premailer in /usr/local/lib/python3.10/dist-packages (from paddleocr) (3.10.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from paddleocr) (3.1.3)\n",
            "Requirement already satisfied: attrdict in /usr/local/lib/python3.10/dist-packages (from paddleocr) (2.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (from paddleocr) (1.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.12.3)\n",
            "Requirement already satisfied: fonttools>=4.24.0 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.53.0)\n",
            "Requirement already satisfied: fire>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.6.0)\n",
            "Requirement already satisfied: pdf2docx in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.5.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (0.27.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (4.4.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.3.0->paddleocr) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.3.0->paddleocr) (2.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.5.40)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->paddleocr) (2.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (2024.5.22)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (1.6.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->paddleocr) (1.1.0)\n",
            "Requirement already satisfied: PyMuPDF>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from pdf2docx->paddleocr) (1.24.5)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5 in /usr/local/lib/python3.10/dist-packages (from pdf2docx->paddleocr) (4.10.0.82)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.10/dist-packages (from premailer->paddleocr) (1.2.0)\n",
            "Requirement already satisfied: cssutils in /usr/local/lib/python3.10/dist-packages (from premailer->paddleocr) (2.11.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from premailer->paddleocr) (5.3.3)\n",
            "Requirement already satisfied: bce-python-sdk in /usr/local/lib/python3.10/dist-packages (from visualdl->paddleocr) (0.9.14)\n",
            "Requirement already satisfied: flask>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from visualdl->paddleocr) (2.2.5)\n",
            "Requirement already satisfied: Flask-Babel>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from visualdl->paddleocr) (4.0.0)\n",
            "Requirement already satisfied: rarfile in /usr/local/lib/python3.10/dist-packages (from visualdl->paddleocr) (4.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.1->visualdl->paddleocr) (3.0.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.1->visualdl->paddleocr) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask>=1.1.1->visualdl->paddleocr) (8.1.7)\n",
            "Requirement already satisfied: Babel>=2.12 in /usr/local/lib/python3.10/dist-packages (from Flask-Babel>=3.0.0->visualdl->paddleocr) (2.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: PyMuPDFb==1.24.3 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF>=1.19.0->pdf2docx->paddleocr) (1.24.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->paddlepaddle) (1.2.1)\n",
            "Requirement already satisfied: pycryptodome>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from bce-python-sdk->visualdl->paddleocr) (3.20.0)\n",
            "Requirement already satisfied: future>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from bce-python-sdk->visualdl->paddleocr) (0.18.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cssutils->premailer->paddleocr) (10.1.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Activation, add, Dense, Flatten, Dropout, Conv2D, AveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "sys.setrecursionlimit(2 ** 20)\n",
        "np.random.seed(2 ** 10)\n",
        "\n",
        "\n",
        "class WideResNet:\n",
        "    def __init__(self, image_size, depth=16, k=8):\n",
        "        self._depth = depth\n",
        "        self._k = k\n",
        "        self._dropout_probability = 0\n",
        "        self._weight_decay = 0.0005\n",
        "        self._use_bias = False\n",
        "        self._weight_init = \"he_normal\"\n",
        "\n",
        "        if K.image_data_format() == \"channels_first\":\n",
        "            logging.debug(\"image_dim_ordering = 'th'\")\n",
        "            self._channel_axis = 1\n",
        "            self._input_shape = (3, image_size, image_size)\n",
        "        else:\n",
        "            logging.debug(\"image_dim_ordering = 'tf'\")\n",
        "            self._channel_axis = -1\n",
        "            self._input_shape = (image_size, image_size, 3)\n",
        "\n",
        "    # Wide residual network http://arxiv.org/abs/1605.07146\n",
        "    def _wide_basic(self, n_input_plane, n_output_plane, stride):\n",
        "        def f(net):\n",
        "            # format of conv_params:\n",
        "            #               [ [kernel_size=(\"kernel width\", \"kernel height\"),\n",
        "            #               strides=\"(stride_vertical,stride_horizontal)\",\n",
        "            #               padding=\"same\" or \"valid\"] ]\n",
        "            # B(3,3): orignal <<basic>> block\n",
        "            conv_params = [[3, 3, stride, \"same\"],\n",
        "                           [3, 3, (1, 1), \"same\"]]\n",
        "\n",
        "            n_bottleneck_plane = n_output_plane\n",
        "\n",
        "            # Residual block\n",
        "            for i, v in enumerate(conv_params):\n",
        "                if i == 0:\n",
        "                    if n_input_plane != n_output_plane:\n",
        "                        net = BatchNormalization(axis=self._channel_axis)(net)\n",
        "                        net = Activation(\"relu\")(net)\n",
        "                        convs = net\n",
        "                    else:\n",
        "                        convs = BatchNormalization(axis=self._channel_axis)(net)\n",
        "                        convs = Activation(\"relu\")(convs)\n",
        "\n",
        "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
        "                                          strides=v[2],\n",
        "                                          padding=v[3],\n",
        "                                          kernel_initializer=self._weight_init,\n",
        "                                          kernel_regularizer=l2(self._weight_decay),\n",
        "                                          use_bias=self._use_bias)(convs)\n",
        "                else:\n",
        "                    convs = BatchNormalization(axis=self._channel_axis)(convs)\n",
        "                    convs = Activation(\"relu\")(convs)\n",
        "                    if self._dropout_probability > 0:\n",
        "                        convs = Dropout(self._dropout_probability)(convs)\n",
        "                    convs = Conv2D(n_bottleneck_plane, kernel_size=(v[0], v[1]),\n",
        "                                          strides=v[2],\n",
        "                                          padding=v[3],\n",
        "                                          kernel_initializer=self._weight_init,\n",
        "                                          kernel_regularizer=l2(self._weight_decay),\n",
        "                                          use_bias=self._use_bias)(convs)\n",
        "\n",
        "            # Shortcut Connection: identity function or 1x1 convolutional\n",
        "            #  (depends on difference between input & output shape - this\n",
        "            #   corresponds to whether we are using the first block in each\n",
        "            #   group; see _layer() ).\n",
        "            if n_input_plane != n_output_plane:\n",
        "                shortcut = Conv2D(n_output_plane, kernel_size=(1, 1),\n",
        "                                         strides=stride,\n",
        "                                         padding=\"same\",\n",
        "                                         kernel_initializer=self._weight_init,\n",
        "                                         kernel_regularizer=l2(self._weight_decay),\n",
        "                                         use_bias=self._use_bias)(net)\n",
        "            else:\n",
        "                shortcut = net\n",
        "\n",
        "            return add([convs, shortcut])\n",
        "\n",
        "        return f\n",
        "\n",
        "\n",
        "    # \"Stacking Residual Units on the same stage\"\n",
        "    def _layer(self, block, n_input_plane, n_output_plane, count, stride):\n",
        "        def f(net):\n",
        "            net = block(n_input_plane, n_output_plane, stride)(net)\n",
        "            for i in range(2, int(count + 1)):\n",
        "                net = block(n_output_plane, n_output_plane, stride=(1, 1))(net)\n",
        "            return net\n",
        "\n",
        "        return f\n",
        "\n",
        "#    def create_model(self):\n",
        "    def __call__(self):\n",
        "        logging.debug(\"Creating model...\")\n",
        "\n",
        "        assert ((self._depth - 4) % 6 == 0)\n",
        "        n = (self._depth - 4) / 6\n",
        "\n",
        "        inputs = Input(shape=self._input_shape)\n",
        "\n",
        "        n_stages = [16, 16 * self._k, 32 * self._k, 64 * self._k]\n",
        "\n",
        "        conv1 = Conv2D(filters=n_stages[0], kernel_size=(3, 3),\n",
        "                              strides=(1, 1),\n",
        "                              padding=\"same\",\n",
        "                              kernel_initializer=self._weight_init,\n",
        "                              kernel_regularizer=l2(self._weight_decay),\n",
        "                              use_bias=self._use_bias)(inputs)  # \"One conv at the beginning (spatial size: 32x32)\"\n",
        "\n",
        "        # Add wide residual blocks\n",
        "        block_fn = self._wide_basic\n",
        "        conv2 = self._layer(block_fn, n_input_plane=n_stages[0], n_output_plane=n_stages[1], count=n, stride=(1, 1))(conv1)\n",
        "        conv3 = self._layer(block_fn, n_input_plane=n_stages[1], n_output_plane=n_stages[2], count=n, stride=(2, 2))(conv2)\n",
        "        conv4 = self._layer(block_fn, n_input_plane=n_stages[2], n_output_plane=n_stages[3], count=n, stride=(2, 2))(conv3)\n",
        "        batch_norm = BatchNormalization(axis=self._channel_axis)(conv4)\n",
        "        relu = Activation(\"relu\")(batch_norm)\n",
        "\n",
        "        # Classifier block\n",
        "        pool = AveragePooling2D(pool_size=(8, 8), strides=(1, 1), padding=\"same\")(relu)\n",
        "        flatten = Flatten()(pool)\n",
        "        predictions_g = Dense(units=2, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
        "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\",\n",
        "                              name=\"pred_gender\")(flatten)\n",
        "        predictions_a = Dense(units=101, kernel_initializer=self._weight_init, use_bias=self._use_bias,\n",
        "                              kernel_regularizer=l2(self._weight_decay), activation=\"softmax\",\n",
        "                              name=\"pred_age\")(flatten)\n",
        "        model = Model(inputs=inputs, outputs=[predictions_g, predictions_a])\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T13:08:29.069801Z",
          "iopub.execute_input": "2024-06-13T13:08:29.070292Z",
          "iopub.status.idle": "2024-06-13T13:08:43.195645Z",
          "shell.execute_reply.started": "2024-06-13T13:08:29.070252Z",
          "shell.execute_reply": "2024-06-13T13:08:43.194679Z"
        },
        "trusted": true,
        "id": "BNCCKnENxVOO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "video_url = 'https://www.youtube.com/watch?v=o4Zd-IeMlSY'\n",
        "\n",
        "yt = YouTube(video_url)\n",
        "\n",
        "video_stream = yt.streams.get_highest_resolution()\n",
        "\n",
        "output_path = 'downloaded_video.mp4'\n",
        "\n",
        "video_stream.download(filename=output_path)\n",
        "\n",
        "print(f\"Video downloaded and saved as {output_path}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T13:22:45.046912Z",
          "iopub.execute_input": "2024-06-13T13:22:45.047465Z",
          "iopub.status.idle": "2024-06-13T13:23:03.848177Z",
          "shell.execute_reply.started": "2024-06-13T13:22:45.04741Z",
          "shell.execute_reply": "2024-06-13T13:23:03.846858Z"
        },
        "trusted": true,
        "id": "Q76EoX8ZxVOO",
        "outputId": "7d97f38d-6849-4b19-efcc-2d5862d720ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video downloaded and saved as downloaded_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from ultralytics import YOLO\n",
        "from paddleocr import PaddleOCR\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import sys\n",
        "import io\n",
        "from transformers import pipeline\n",
        "import hashlib\n",
        "\n",
        "class WrapperClass:\n",
        "    def __init__(self, img_size, depth, k, margin):\n",
        "        self.img_size = img_size\n",
        "        self.margin = margin\n",
        "        self.ag_model = WideResNet(img_size, depth=depth, k=k)()\n",
        "        self.ag_model.load_weights(\"/kaggle/input/emogen2/tensorflow2/one/1/weights.28-3.73.hdf5\")\n",
        "        self.emotion_model = load_model(\"/kaggle/input/emogen2/tensorflow2/one/1/emotion_little_vgg_2.h5\")\n",
        "        self.yolo = YOLO(\"/kaggle/input/yolov8/pytorch/open-image-v8/1/yolov8s-oiv7.pt\")\n",
        "        self.ocr = PaddleOCR(use_angle_cls=True, lang='en') # need to run only once to download and load model into memory\n",
        "        self.emotion_classes = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}\n",
        "        self.detector = dlib.get_frontal_face_detector()\n",
        "        self.corrector = pipeline('text2text-generation', model='vennify/t5-base-grammar-correction')\n",
        "        self.prev_frame = None\n",
        "\n",
        "    def hash_frame(self, frame):\n",
        "        frame_bytes = frame.tobytes()\n",
        "        return hashlib.md5(frame_bytes).hexdigest()\n",
        "\n",
        "    def detection(self, frame):\n",
        "        return self.detector(frame, 1)\n",
        "\n",
        "    def preprocess_face(self, img_h, img_w, detected, faces, frame):\n",
        "        preprocessed_faces_emo = []\n",
        "        if len(detected) > 0:\n",
        "            for i, d in enumerate(detected):\n",
        "                x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
        "                xw1 = max(int(x1 - self.margin * w), 0)\n",
        "                yw1 = max(int(y1 - self.margin * h), 0)\n",
        "                xw2 = min(int(x2 + self.margin * w), img_w - 1)\n",
        "                yw2 = min(int(y2 + self.margin * h), img_h - 1)\n",
        "                faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (self.img_size, self.img_size))\n",
        "                face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
        "                face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "                face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation = cv2.INTER_AREA)\n",
        "                face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\n",
        "                face_gray_emo = img_to_array(face_gray_emo)\n",
        "                face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\n",
        "                preprocessed_faces_emo.append(face_gray_emo)\n",
        "\n",
        "        return preprocessed_faces_emo, faces\n",
        "\n",
        "    def predict_ages(self, detected, faces):\n",
        "        if len(detected) > 0:\n",
        "            results = self.ag_model.predict(np.array(faces))\n",
        "            predicted_genders = results[0]\n",
        "            predicted_ages = np.argsort(results[1])[0][-5:]\n",
        "            age_range = (min(predicted_ages), max(predicted_ages))\n",
        "\n",
        "            return predicted_genders, age_range\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def predict_emo(self, detected, preprocessed_faces_emo):\n",
        "        emo_labels = []\n",
        "        for i, d in enumerate(detected):\n",
        "            preds = self.emotion_model.predict(preprocessed_faces_emo[i])[0]\n",
        "            e = self.emotion_classes[preds.argmax()]\n",
        "            emo_labels.append(e)\n",
        "        return emo_labels\n",
        "\n",
        "    def yolo_pred(self, frame):\n",
        "        res = self.yolo(frame)\n",
        "        return [(i['name'], i['box']) for j in res for i in j.summary()]\n",
        "\n",
        "    def correct_text(self, text):\n",
        "        return self.corrector(text)[0]['generated_text']\n",
        "\n",
        "\n",
        "    def vision_pred(self, frame):\n",
        "        preprocessed_faces_emo = []\n",
        "        input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        img_h, img_w, _ = np.shape(input_img)\n",
        "        detected = self.detector(frame, 1)\n",
        "        faces = np.empty((len(detected), self.img_size, self.img_size, 3))\n",
        "        preprocessed_faces_emo, faces = self.preprocess_face(img_h, img_w, detected, faces, frame)\n",
        "        gender, age = self.predict_ages(detected, faces)\n",
        "        emo_labels = self.predict_emo(detected, preprocessed_faces_emo)\n",
        "        print(gender, age, emo_labels)\n",
        "        result = self.yolo_pred(frame)\n",
        "        print(result)\n",
        "\n",
        "    def ocr_pred(self, frame):\n",
        "        frame_hash = self.hash_frame(frame)\n",
        "        if frame_hash == self.prev_frame:\n",
        "            print(\"skipping frame\")\n",
        "            return\n",
        "        result = self.ocr.ocr(frame, cls=True)\n",
        "        detected_text = \"\"\n",
        "        for idx in range(len(result)):\n",
        "            res = result[idx]\n",
        "            if res:\n",
        "                for line in res:\n",
        "                    detected_text += \" \" + line[1][0]\n",
        "        # corrected_text = self.correct_text(detected_text)\n",
        "        print(detected_text)\n",
        "        self.prev_frame = frame_hash\n",
        "\n",
        "    def make_prediction(self, video_path, func):\n",
        "        a = time.time()\n",
        "        output_video_path = 'output.mp4'\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        frame_count = 0\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % 30 != 0:\n",
        "                frame_count += 1\n",
        "                continue\n",
        "            if frame_count == 600:\n",
        "              break\n",
        "\n",
        "            print(frame_count)\n",
        "            func(frame)\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        b = time.time()\n",
        "        print(b - a)\n",
        "\n",
        "    def __call__(self, video_path, func):\n",
        "        if func == \"see\":\n",
        "            self.make_prediction(video_path, self.vision_pred)\n",
        "        elif func == \"ocr\":\n",
        "            self.make_prediction(video_path, self.ocr_pred)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T13:23:07.48037Z",
          "iopub.execute_input": "2024-06-13T13:23:07.481119Z",
          "iopub.status.idle": "2024-06-13T13:23:14.254535Z",
          "shell.execute_reply.started": "2024-06-13T13:23:07.481081Z",
          "shell.execute_reply": "2024-06-13T13:23:14.253314Z"
        },
        "trusted": true,
        "id": "fOvej95BxVOP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import warnings\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ZfQ_Z1piDZNJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = WrapperClass(64, 16, 8, 0.3)\n",
        "a(\"/content/downloaded_video.mp4\", \"see\")"
      ],
      "metadata": {
        "id": "oNTKZwC8xVOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d848b767-5f6c-4180-cb1b-114db298d3aa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024/06/14 01:15:10] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/usr/local/lib/python3.10/dist-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
            "0\n",
            "1/1 [==============================] - 0s 497ms/step\n",
            "1/1 [==============================] - 0s 279ms/step\n",
            "[[  0.0013018      0.9987]] (26, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 400.9ms\n",
            "Speed: 2.6ms preprocess, 400.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 561.53149, 'y1': 171.50504, 'x2': 758.02905, 'y2': 477.70853}), ('Man', {'x1': 228.54907, 'y1': 96.98566, 'x2': 998.32617, 'y2': 717.63141}), ('Chair', {'x1': 798.90979, 'y1': 432.22934, 'x2': 1139.91052, 'y2': 719.86267}), ('Houseplant', {'x1': 1095.89551, 'y1': 325.46219, 'x2': 1280.0, 'y2': 719.4231}), ('Microphone', {'x1': 234.26419, 'y1': 421.36841, 'x2': 495.71494, 'y2': 639.00708}), ('Clothing', {'x1': 241.07996, 'y1': 407.42761, 'x2': 993.14587, 'y2': 717.78442})]\n",
            "30\n",
            "1/1 [==============================] - 0s 214ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "[[  0.0067195     0.99328]] (22, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 346.2ms\n",
            "Speed: 6.1ms preprocess, 346.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 535.04175, 'y1': 163.64868, 'x2': 728.52319, 'y2': 460.62744}), ('Man', {'x1': 236.29211, 'y1': 94.12842, 'x2': 980.37, 'y2': 715.79871}), ('Chair', {'x1': 804.26721, 'y1': 432.41754, 'x2': 1140.04797, 'y2': 719.72858}), ('Houseplant', {'x1': 1092.98657, 'y1': 325.69589, 'x2': 1280.0, 'y2': 719.29578}), ('Microphone', {'x1': 234.52881, 'y1': 420.89673, 'x2': 496.28345, 'y2': 637.54236}), ('Clothing', {'x1': 252.41867, 'y1': 405.21362, 'x2': 979.51562, 'y2': 717.15845})]\n",
            "60\n",
            "1/1 [==============================] - 0s 359ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "[[    0.00688     0.99312]] (22, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 530.9ms\n",
            "Speed: 2.7ms preprocess, 530.9ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 526.58905, 'y1': 166.38287, 'x2': 718.33368, 'y2': 462.04456}), ('Man', {'x1': 235.99164, 'y1': 97.69513, 'x2': 980.97333, 'y2': 715.59998}), ('Chair', {'x1': 802.2666, 'y1': 432.51086, 'x2': 1140.125, 'y2': 719.76062}), ('Houseplant', {'x1': 1094.06543, 'y1': 325.55664, 'x2': 1280.0, 'y2': 719.25732}), ('Microphone', {'x1': 237.35394, 'y1': 421.25174, 'x2': 494.40283, 'y2': 638.30005}), ('Clothing', {'x1': 255.80072, 'y1': 408.17911, 'x2': 980.76801, 'y2': 716.86365})]\n",
            "90\n",
            "None None []\n",
            "\n",
            "0: 384x640 1 Car, 599.2ms\n",
            "Speed: 2.6ms preprocess, 599.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Car', {'x1': 81.93066, 'y1': 269.72754, 'x2': 480.40479, 'y2': 551.6521})]\n",
            "120\n",
            "None None []\n",
            "\n",
            "0: 384x640 3 Cars, 342.5ms\n",
            "Speed: 4.4ms preprocess, 342.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Car', {'x1': 857.38721, 'y1': 224.97983, 'x2': 1193.61816, 'y2': 476.01389}), ('Car', {'x1': 76.62994, 'y1': 225.37123, 'x2': 455.44855, 'y2': 670.745}), ('Car', {'x1': 321.7218, 'y1': 225.5724, 'x2': 539.25696, 'y2': 411.67529})]\n",
            "150\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[    0.25345     0.74655]] (26, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Man, 1 Microphone, 1 Person, 353.1ms\n",
            "Speed: 3.1ms preprocess, 353.1ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 529.20837, 'y1': 184.83543, 'x2': 727.3197, 'y2': 480.18909}), ('Man', {'x1': 241.81873, 'y1': 105.46722, 'x2': 964.43091, 'y2': 717.38147}), ('Person', {'x1': 223.021, 'y1': 105.29761, 'x2': 967.14563, 'y2': 718.35083}), ('Clothing', {'x1': 254.71271, 'y1': 416.41922, 'x2': 963.88751, 'y2': 717.62109}), ('Microphone', {'x1': 238.1142, 'y1': 421.9223, 'x2': 496.3537, 'y2': 639.92511})]\n",
            "180\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[   0.023818     0.97618]] (26, 32) ['Sad']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 362.1ms\n",
            "Speed: 7.4ms preprocess, 362.1ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 535.11938, 'y1': 180.85004, 'x2': 732.08252, 'y2': 476.953}), ('Man', {'x1': 242.71033, 'y1': 111.88617, 'x2': 969.36804, 'y2': 717.05676}), ('Chair', {'x1': 807.27527, 'y1': 431.78574, 'x2': 1139.1925, 'y2': 719.63977}), ('Houseplant', {'x1': 1097.57251, 'y1': 324.32214, 'x2': 1280.0, 'y2': 719.46606}), ('Clothing', {'x1': 250.21863, 'y1': 408.45181, 'x2': 968.87842, 'y2': 718.72083})]\n",
            "210\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[     0.0134      0.9866]] (27, 33) ['Happy']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Man, 1 Microphone, 356.3ms\n",
            "Speed: 2.5ms preprocess, 356.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 540.633, 'y1': 174.10007, 'x2': 734.19562, 'y2': 473.59024}), ('Man', {'x1': 248.20108, 'y1': 109.39081, 'x2': 936.79236, 'y2': 715.41882}), ('Clothing', {'x1': 263.06934, 'y1': 409.42166, 'x2': 924.99512, 'y2': 716.72656}), ('Microphone', {'x1': 235.47639, 'y1': 421.09351, 'x2': 497.36395, 'y2': 639.39331})]\n",
            "240\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[   0.040341     0.95966]] (25, 32) ['Sad']\n",
            "\n",
            "0: 384x640 1 Clothing, 1 Human face, 1 Man, 2 Microphones, 1 Person, 359.7ms\n",
            "Speed: 4.4ms preprocess, 359.7ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 539.85852, 'y1': 179.65353, 'x2': 737.9679, 'y2': 478.49808}), ('Microphone', {'x1': 232.89903, 'y1': 421.53644, 'x2': 495.29138, 'y2': 640.5163}), ('Man', {'x1': 223.2804, 'y1': 104.35815, 'x2': 943.13721, 'y2': 714.99622}), ('Clothing', {'x1': 261.0647, 'y1': 411.5426, 'x2': 930.51501, 'y2': 716.62793}), ('Person', {'x1': 77.20184, 'y1': 101.12476, 'x2': 946.46063, 'y2': 717.35156}), ('Microphone', {'x1': 3.7363, 'y1': 311.47168, 'x2': 365.72614, 'y2': 489.43774})]\n",
            "270\n",
            "1/1 [==============================] - 0s 230ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "[[  0.0062391     0.99376]] (22, 32) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 345.6ms\n",
            "Speed: 4.5ms preprocess, 345.6ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 550.07928, 'y1': 174.33875, 'x2': 738.85443, 'y2': 470.67078}), ('Man', {'x1': 225.67603, 'y1': 102.72253, 'x2': 992.93469, 'y2': 716.45337}), ('Chair', {'x1': 798.05347, 'y1': 431.81912, 'x2': 1139.66284, 'y2': 719.80493}), ('Houseplant', {'x1': 1096.6488, 'y1': 324.65335, 'x2': 1280.0, 'y2': 719.25317}), ('Clothing', {'x1': 240.81226, 'y1': 408.19421, 'x2': 991.3927, 'y2': 717.50745}), ('Microphone', {'x1': 231.50731, 'y1': 420.85623, 'x2': 496.04578, 'y2': 639.34644})]\n",
            "300\n",
            "1/1 [==============================] - 0s 212ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[   0.023731     0.97627]] (29, 37) ['Sad']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 344.0ms\n",
            "Speed: 5.0ms preprocess, 344.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 542.85791, 'y1': 180.49127, 'x2': 740.75195, 'y2': 480.48688}), ('Man', {'x1': 206.82385, 'y1': 105.6333, 'x2': 995.83447, 'y2': 715.26135}), ('Chair', {'x1': 806.2666, 'y1': 431.57428, 'x2': 1139.33691, 'y2': 719.97235}), ('Houseplant', {'x1': 1097.29639, 'y1': 325.49176, 'x2': 1280.0, 'y2': 719.09808}), ('Clothing', {'x1': 225.81705, 'y1': 407.67099, 'x2': 994.36267, 'y2': 717.07068})]\n",
            "330\n",
            "1/1 [==============================] - 0s 219ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[   0.025328     0.97467]] (29, 37) ['Sad']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 485.5ms\n",
            "Speed: 5.5ms preprocess, 485.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 536.89282, 'y1': 184.189, 'x2': 736.2124, 'y2': 482.28201}), ('Man', {'x1': 218.26645, 'y1': 105.87439, 'x2': 997.85535, 'y2': 715.48547}), ('Chair', {'x1': 814.34192, 'y1': 432.24426, 'x2': 1139.81006, 'y2': 719.88318}), ('Houseplant', {'x1': 1098.21423, 'y1': 323.83472, 'x2': 1280.0, 'y2': 719.22998}), ('Clothing', {'x1': 246.50537, 'y1': 406.88901, 'x2': 996.14746, 'y2': 716.97131}), ('Microphone', {'x1': 234.54529, 'y1': 421.30115, 'x2': 496.35449, 'y2': 636.79663})]\n",
            "360\n",
            "1/1 [==============================] - 0s 377ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "[[  0.0060953      0.9939]] (26, 33) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Houseplant, 1 Human face, 1 Man, 554.8ms\n",
            "Speed: 2.7ms preprocess, 554.8ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 550.9071, 'y1': 189.92889, 'x2': 748.72998, 'y2': 492.26172}), ('Man', {'x1': 109.05762, 'y1': 102.90277, 'x2': 984.24121, 'y2': 717.06451}), ('Chair', {'x1': 808.55347, 'y1': 432.36575, 'x2': 1140.0625, 'y2': 719.91357}), ('Houseplant', {'x1': 1098.52844, 'y1': 323.79361, 'x2': 1280.0, 'y2': 719.29211})]\n",
            "390\n",
            "1/1 [==============================] - 0s 418ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[[  0.0052839     0.99472]] (29, 33) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 350.3ms\n",
            "Speed: 3.6ms preprocess, 350.3ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Human face', {'x1': 528.12305, 'y1': 164.29205, 'x2': 718.24866, 'y2': 453.10455}), ('Man', {'x1': 242.65155, 'y1': 98.6012, 'x2': 1005.15643, 'y2': 716.20825}), ('Chair', {'x1': 814.14893, 'y1': 432.36108, 'x2': 1139.57483, 'y2': 719.91162}), ('Houseplant', {'x1': 1097.71411, 'y1': 324.06622, 'x2': 1280.0, 'y2': 719.19745}), ('Microphone', {'x1': 237.42999, 'y1': 421.50122, 'x2': 496.27795, 'y2': 638.39636}), ('Clothing', {'x1': 252.48743, 'y1': 405.98303, 'x2': 1004.31006, 'y2': 717.60901})]\n",
            "420\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "[[  0.0049428     0.99506]] (26, 35) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 1 Microphone, 352.8ms\n",
            "Speed: 5.1ms preprocess, 352.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Man', {'x1': 208.94308, 'y1': 94.77161, 'x2': 1002.76904, 'y2': 715.23425}), ('Human face', {'x1': 557.01782, 'y1': 166.55075, 'x2': 754.85938, 'y2': 467.91965}), ('Chair', {'x1': 809.23975, 'y1': 432.60492, 'x2': 1140.9585, 'y2': 719.97614}), ('Houseplant', {'x1': 1098.49463, 'y1': 323.88531, 'x2': 1280.0, 'y2': 719.19574}), ('Clothing', {'x1': 240.81787, 'y1': 412.75256, 'x2': 1001.61475, 'y2': 716.99109}), ('Microphone', {'x1': 231.32642, 'y1': 421.10736, 'x2': 495.93274, 'y2': 639.2254})]\n",
            "450\n",
            "None None []\n",
            "\n",
            "0: 384x640 1 Computer monitor, 1 Tablet computer, 349.0ms\n",
            "Speed: 4.5ms preprocess, 349.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Computer monitor', {'x1': 36.91772, 'y1': 28.8222, 'x2': 1228.13965, 'y2': 701.953}), ('Tablet computer', {'x1': 26.43079, 'y1': 28.32697, 'x2': 1237.55127, 'y2': 701.04596})]\n",
            "480\n",
            "1/1 [==============================] - 0s 211ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[       0.33        0.67]] (35, 43) ['Surprise']\n",
            "\n",
            "0: 384x640 1 Computer monitor, 364.1ms\n",
            "Speed: 4.5ms preprocess, 364.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Computer monitor', {'x1': 10.49792, 'y1': 33.71936, 'x2': 1225.63574, 'y2': 699.35571})]\n",
            "510\n",
            "None None []\n",
            "\n",
            "0: 384x640 1 Computer monitor, 366.8ms\n",
            "Speed: 4.9ms preprocess, 366.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Computer monitor', {'x1': 15.93909, 'y1': 29.91199, 'x2': 1234.74182, 'y2': 695.5824})]\n",
            "540\n",
            "None None []\n",
            "\n",
            "0: 384x640 1 Tablet computer, 356.7ms\n",
            "Speed: 2.8ms preprocess, 356.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Tablet computer', {'x1': 45.46997, 'y1': 22.03412, 'x2': 1247.11621, 'y2': 697.74841})]\n",
            "570\n",
            "1/1 [==============================] - 0s 216ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "[[  0.0073046      0.9927]] (21, 29) ['Happy']\n",
            "\n",
            "0: 384x640 1 Chair, 1 Clothing, 1 Houseplant, 1 Human face, 1 Man, 2 Microphones, 353.9ms\n",
            "Speed: 2.5ms preprocess, 353.9ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "[('Chair', {'x1': 810.08228, 'y1': 432.48065, 'x2': 1140.00244, 'y2': 719.87909}), ('Human face', {'x1': 504.80798, 'y1': 162.14955, 'x2': 699.06165, 'y2': 450.51672}), ('Man', {'x1': 224.25787, 'y1': 99.55237, 'x2': 998.1767, 'y2': 715.18665}), ('Houseplant', {'x1': 1094.58813, 'y1': 324.19962, 'x2': 1280.0, 'y2': 719.09045}), ('Microphone', {'x1': 237.71892, 'y1': 421.27737, 'x2': 496.8819, 'y2': 637.13062}), ('Clothing', {'x1': 256.62964, 'y1': 405.60443, 'x2': 995.05615, 'y2': 717.34015}), ('Microphone', {'x1': 178.67438, 'y1': 317.18268, 'x2': 364.06207, 'y2': 495.11279})]\n",
            "31.44351816177368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a(\"/content/downloaded_video.mp4\", \"ocr\")"
      ],
      "metadata": {
        "id": "mTgcSm8UC6m8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77cf64d8-d74e-4069-eff0-5b41a231fa16"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "[2024/06/14 01:14:07] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.1559889316558838\n",
            "[2024/06/14 01:14:07] ppocr DEBUG: cls num  : 1, elapsed : 0.016289472579956055\n",
            "[2024/06/14 01:14:08] ppocr DEBUG: rec_res num  : 1, elapsed : 0.0923764705657959\n",
            "\n",
            "30\n",
            "[2024/06/14 01:14:08] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.16633033752441406\n",
            "[2024/06/14 01:14:08] ppocr DEBUG: cls num  : 1, elapsed : 0.009167909622192383\n",
            "[2024/06/14 01:14:08] ppocr DEBUG: rec_res num  : 1, elapsed : 0.050390005111694336\n",
            "\n",
            "60\n",
            "[2024/06/14 01:14:09] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.2316126823425293\n",
            "[2024/06/14 01:14:09] ppocr DEBUG: cls num  : 2, elapsed : 0.02582716941833496\n",
            "[2024/06/14 01:14:09] ppocr DEBUG: rec_res num  : 2, elapsed : 0.14336776733398438\n",
            "PEASETP.\n",
            "90\n",
            "[2024/06/14 01:14:10] ppocr DEBUG: dt_boxes num : 5, elapsed : 0.22837615013122559\n",
            "[2024/06/14 01:14:10] ppocr DEBUG: cls num  : 5, elapsed : 0.05631256103515625\n",
            "[2024/06/14 01:14:11] ppocr DEBUG: rec_res num  : 5, elapsed : 0.5270974636077881\n",
            "Car 0.94 CCT 0.car 0.69Ccar 0.90 DAJ46D.\n",
            "120\n",
            "[2024/06/14 01:14:12] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.16124916076660156\n",
            "[2024/06/14 01:14:12] ppocr DEBUG: cls num  : 3, elapsed : 0.01884007453918457\n",
            "[2024/06/14 01:14:13] ppocr DEBUG: rec_res num  : 3, elapsed : 0.2432563304901123\n",
            "Car 0.93 car nR9 car 0.81ar 0car 0.car 0.94\n",
            "150\n",
            "[2024/06/14 01:14:14] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.1582181453704834\n",
            "[2024/06/14 01:14:14] ppocr DEBUG: cls num  : 2, elapsed : 0.017557382583618164\n",
            "[2024/06/14 01:14:14] ppocr DEBUG: rec_res num  : 2, elapsed : 0.10813641548156738\n",
            "Ultralytics YOLOv8 YOLOv8 YOLO\n",
            "180\n",
            "[2024/06/14 01:14:16] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.1575477123260498\n",
            "[2024/06/14 01:14:16] ppocr DEBUG: cls num  : 2, elapsed : 0.017002105712890625\n",
            "[2024/06/14 01:14:16] ppocr DEBUG: rec_res num  : 2, elapsed : 0.0918118953704834\n",
            "Tomorrow, the world will be a better place.\n",
            "210\n",
            "[2024/06/14 01:14:17] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.16415882110595703\n",
            "[2024/06/14 01:14:17] ppocr DEBUG: cls num  : 3, elapsed : 0.012192249298095703\n",
            "[2024/06/14 01:14:17] ppocr DEBUG: rec_res num  : 3, elapsed : 0.1342148780822754\n",
            "Ultralytics SHAPI SHAPI ultralytics SHAPI ultralytics \n",
            "240\n",
            "[2024/06/14 01:14:19] ppocr DEBUG: dt_boxes num : 3, elapsed : 0.15918421745300293\n",
            "[2024/06/14 01:14:19] ppocr DEBUG: cls num  : 3, elapsed : 0.012708187103271484\n",
            "[2024/06/14 01:14:19] ppocr DEBUG: rec_res num  : 3, elapsed : 0.11532044410705566\n",
            "Ultralytics SHAPING NEW TOMORROW.\n",
            "270\n",
            "[2024/06/14 01:14:20] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.16079950332641602\n",
            "[2024/06/14 01:14:20] ppocr DEBUG: cls num  : 1, elapsed : 0.0121002197265625\n",
            "[2024/06/14 01:14:20] ppocr DEBUG: rec_res num  : 1, elapsed : 0.061415910720825195\n",
            "\n",
            "300\n",
            "[2024/06/14 01:14:20] ppocr DEBUG: dt_boxes num : 1, elapsed : 0.15838122367858887\n",
            "[2024/06/14 01:14:20] ppocr DEBUG: cls num  : 1, elapsed : 0.010374069213867188\n",
            "[2024/06/14 01:14:21] ppocr DEBUG: rec_res num  : 1, elapsed : 0.06927919387817383\n",
            "\n",
            "330\n",
            "[2024/06/14 01:14:21] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.16830825805664062\n",
            "[2024/06/14 01:14:21] ppocr DEBUG: cls num  : 2, elapsed : 0.018620967864990234\n",
            "[2024/06/14 01:14:21] ppocr DEBUG: rec_res num  : 2, elapsed : 0.09563612937927246\n",
            "Newtworrow, the world's largest earthquake, is coming to an end.\n",
            "360\n",
            "[2024/06/14 01:14:23] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.2323741912841797\n",
            "[2024/06/14 01:14:23] ppocr DEBUG: cls num  : 2, elapsed : 0.022960662841796875\n",
            "[2024/06/14 01:14:23] ppocr DEBUG: rec_res num  : 2, elapsed : 0.13212990760803223\n",
            "WewTONORROW, we will be celebrating the World's Greatest Day.\n",
            "390\n",
            "[2024/06/14 01:14:25] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.22765040397644043\n",
            "[2024/06/14 01:14:25] ppocr DEBUG: cls num  : 2, elapsed : 0.017420053482055664\n",
            "[2024/06/14 01:14:25] ppocr DEBUG: rec_res num  : 2, elapsed : 0.09378314018249512\n",
            "SHAPINGA - SHAPINGA - SHAPINGA - SH\n",
            "420\n",
            "[2024/06/14 01:14:27] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.16226482391357422\n",
            "[2024/06/14 01:14:27] ppocr DEBUG: cls num  : 2, elapsed : 0.016695737838745117\n",
            "[2024/06/14 01:14:27] ppocr DEBUG: rec_res num  : 2, elapsed : 0.1022179126739502\n",
            "The new tomorrow will be a new day.\n",
            "450\n",
            "[2024/06/14 01:14:28] ppocr DEBUG: dt_boxes num : 43, elapsed : 0.1724705696105957\n",
            "[2024/06/14 01:14:28] ppocr DEBUG: cls num  : 43, elapsed : 0.12796449661254883\n",
            "[2024/06/14 01:14:32] ppocr DEBUG: rec_res num  : 43, elapsed : 4.04281210899353\n",
            "The YOLOv8 Language is designed to be fast, accurate, and easy to\n",
            "480\n",
            "[2024/06/14 01:14:35] ppocr DEBUG: dt_boxes num : 52, elapsed : 0.17453670501708984\n",
            "[2024/06/14 01:14:35] ppocr DEBUG: cls num  : 52, elapsed : 0.1439955234527588\n",
            "[2024/06/14 01:14:40] ppocr DEBUG: rec_res num  : 52, elapsed : 4.847261667251587\n",
            "The resources here will help you get the most out of YOLOv8. Please\n",
            "510\n",
            "[2024/06/14 01:14:42] ppocr DEBUG: dt_boxes num : 26, elapsed : 0.16451168060302734\n",
            "[2024/06/14 01:14:42] ppocr DEBUG: cls num  : 26, elapsed : 0.08494901657104492\n",
            "[2024/06/14 01:14:46] ppocr DEBUG: rec_res num  : 26, elapsed : 3.2997474670410156\n",
            "YOLOv8 may be used for a variety of tasks and modes and accept\n",
            "540\n",
            "[2024/06/14 01:14:48] ppocr DEBUG: dt_boxes num : 54, elapsed : 0.17564010620117188\n",
            "[2024/06/14 01:14:48] ppocr DEBUG: cls num  : 54, elapsed : 0.1461787223815918\n",
            "[2024/06/14 01:14:53] ppocr DEBUG: rec_res num  : 54, elapsed : 5.219801902770996\n",
            "The YOLOv8 model supports several modes that can be used to perform different tasks\n",
            "570\n",
            "[2024/06/14 01:14:57] ppocr DEBUG: dt_boxes num : 2, elapsed : 0.2548055648803711\n",
            "[2024/06/14 01:14:57] ppocr DEBUG: cls num  : 2, elapsed : 0.021381139755249023\n",
            "[2024/06/14 01:14:57] ppocr DEBUG: rec_res num  : 2, elapsed : 0.19669437408447266\n",
            "The new tomorrow will be a new day.\n",
            "51.24534749984741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tF0sG6vGRM0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}